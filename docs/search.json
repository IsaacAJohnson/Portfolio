[
  {
    "objectID": "posts/Tables/Tables.html",
    "href": "posts/Tables/Tables.html",
    "title": "Tables NHANES",
    "section": "",
    "text": "Load libraries and data\n\n\nShow the code\nlibrary(gt)\nlibrary(flextable)\nlibrary(DT)\nlibrary(tidyverse)\nlibrary(NHANES)\n\ndata(NHANES)\n\n\nCreate a summary of BMI by age group\n\n\nShow the code\nbmi_data_by_age <- NHANES %>%\n  mutate(AgeGroup = case_when(\n    Age < 18 ~ \"Under 18\",\n    Age >= 18 & Age < 35 ~ \"18-34\",\n    Age >= 35 & Age < 50 ~ \"35-49\",\n    Age >= 50 ~ \"50+\"\n  )) %>%\n  group_by(AgeGroup) %>%\n  summarize(mean_bmi = mean(BMI, na.rm = TRUE)) %>%\n    mutate(mean_bmi = round(mean_bmi, digits = 0))\n\n\nGT table\n\n\nShow the code\ngt_table <- bmi_data_by_age %>%\n  gt() %>%\n  tab_header(title = \"Average BMI by Age Group\") %>%\n  cols_label(\n    AgeGroup = \"Age Group\",\n    mean_bmi = \"Mean BMI\"\n  )\n\n#gt::gt_preview(gt_table)\ngt_table\n\n\n\n\n\n\n  \n    \n      Average BMI by Age Group\n    \n    \n    \n      Age Group\n      Mean BMI\n    \n  \n  \n    18-34\n28\n    35-49\n29\n    50+\n29\n    Under 18\n20\n  \n  \n  \n\n\n\n\nFlextable\n\n\nShow the code\nflextable_table <- bmi_data_by_age %>%\n  flextable() %>%\n  set_caption(\"Average BMI by Age Group\") %>%\n  colformat_num(\n    j = \"mean_bmi\",\n    digits = 2,\n    big.mark = \",\"\n  )\n\n#print(flextable_table)\nflextable_table\n\n\n\nAgeGroupmean_bmi18-342835-492950+29Under 1820\n\n\nDT data table\n\n\nShow the code\n# datatable_table <- datatable(bmi_data_by_age,\n#                              caption = \"Average BMI by Age Group\",\n#                              rownames = FALSE,\n#                              options = list(pageLength = 10))\n# \n# datatable_table\n\n\nAdd more variables\n\n\nShow the code\ncomplex_data_by_age <- NHANES %>%\n  mutate(AgeGroup = factor(case_when(\n    Age < 18 ~ \"Under 18\",\n    Age >= 18 & Age < 35 ~ \"18-34\",\n    Age >= 35 & Age < 50 ~ \"35-49\",\n    Age >= 50 ~ \"50+\"\n  ), levels = c(\"Under 18\", \"18-34\", \"35-49\", \"50+\"))) %>%\n  group_by(AgeGroup) %>%\n  summarize(\n    mean_bmi = mean(BMI, na.rm = TRUE),\n    mean_systolic_BP = mean(BPSysAve, na.rm = TRUE),\n    mean_diastolic_BP = mean(BPDiaAve, na.rm = TRUE),\n    diabetes_prevalence = mean(Diabetes == \"Yes\", na.rm = TRUE) * 100\n  ) %>%\n    mutate(mean_bmi = round(mean_bmi, digits = 0)) %>%\n    mutate(mean_systolic_BP = round(mean_systolic_BP, digits = 0)) %>%\n    mutate(mean_diastolic_BP = round(mean_diastolic_BP, digits = 0)) %>%\n    mutate(diabetes_prevalence = round(diabetes_prevalence, digits = 0))\n\n\nUpdated GT table\n\n\nShow the code\ngt_table_complex <- complex_data_by_age %>%\n  gt() %>%\n  tab_header(title = \"Health Metrics by Age Group\") %>%\n  cols_label(\n    AgeGroup = \"Age Group\",\n    mean_bmi = \"Mean BMI\",\n    mean_systolic_BP = \"Mean Systolic BP\",\n    mean_diastolic_BP = \"Mean Diastolic BP\",\n    diabetes_prevalence = \"Diabetes Prevalence (%)\"\n  )\n\n#gt::gt_preview(gt_table_complex)\ngt_table_complex\n\n\n\n\n\n\n  \n    \n      Health Metrics by Age Group\n    \n    \n    \n      Age Group\n      Mean BMI\n      Mean Systolic BP\n      Mean Diastolic BP\n      Diabetes Prevalence (%)\n    \n  \n  \n    Under 18\n20\n104\n55\n1\n    18-34\n28\n113\n67\n1\n    35-49\n29\n118\n73\n6\n    50+\n29\n128\n70\n19\n  \n  \n  \n\n\n\n\nUpdated Flextable\n\n\nShow the code\nflextable_table_complex <- complex_data_by_age %>%\n  flextable() %>%\n  set_caption(\"Health Metrics by Age Group\") %>%\n  colformat_num(\n    j = c(\"mean_bmi\", \"mean_systolic_BP\", \"mean_diastolic_BP\", \"diabetes_prevalence\"),\n    digits = 2,\n    big.mark = \",\"\n  )\n\n#print(flextable_table_complex)\nflextable_table_complex\n\n\n\nAgeGroupmean_bmimean_systolic_BPmean_diastolic_BPdiabetes_prevalenceUnder 182010455118-342811367135-492911873650+291287019\n\n\nShow the code\n#save_as_image(x = flextable_table_complex, path = \"/Users/isaacjohnson/Willamette/Portfolio/posts/Tables/Tables.png\")\n\n\nUpdated DT data table\n\n\nShow the code\n# datatable_table_complex <- datatable(complex_data_by_age,\n#                                      caption = \"Health Metrics by Age Group\",\n#                                      rownames = FALSE,\n#                                      options = list(pageLength = 10))\n# \n# datatable_table_complex"
  },
  {
    "objectID": "posts/Murder Rates/Murder Rates.html",
    "href": "posts/Murder Rates/Murder Rates.html",
    "title": "Murder Rates",
    "section": "",
    "text": "Load Library and prep data\n\n\nShow the code\nlibrary(maps)\nlibrary(viridis)\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(sf)\nlibrary(tigris)\n\n# Load the USArrests dataset\ndata(USArrests)\n\n# Prepare the data for leaflet\narrests_data <- USArrests %>%\n  rownames_to_column(var = \"state\") %>%\n  mutate(state = tolower(state))\n\n# Load the US states map data\nus_states_map <- map_data(\"state\")\n\n# Define color palette\ncolor_pal <- colorBin(viridis_pal()(5), domain = arrests_data$Murder, bins = 5)\n\n# Join the US states map data with the arrests data\nus_states_map_data <- us_states_map %>%\n  left_join(arrests_data, by = c(\"region\" = \"state\"))\n\n# Load the US states shapefile data\nus_states_shapefile <- states(class = \"sf\", resolution = \"20m\") %>%\n  st_transform(4326) %>%\n  mutate(state = tolower(NAME))\n\n# Join the US states shapefile data with the arrests data\nus_states_map_data <- us_states_shapefile %>%\n  left_join(arrests_data, by = c(\"state\" = \"state\"))\n\n\nCreate Leaflet Map\n\n\nShow the code\n# \n# # Create the leaflet map\n# leaflet_map <- leaflet() %>%\n#   addProviderTiles(\"CartoDB.Positron\") %>%\n#   addPolygons(\n#     data = us_states_map_data,\n#     group = \"states\",\n#     fillColor = ~color_pal(Murder),\n#     color = \"#BDBDC3\",\n#     fillOpacity = 0.7,\n#     weight = 1,\n#     label = ~paste0(NAME, \": \", Murder, \" murders per 100,000\"),\n#     labelOptions = labelOptions(direction = \"auto\")\n#   ) %>%\n#   addLegend(\n#     pal = color_pal,\n#     values = arrests_data$Murder,\n#     title = \"Murders per 100,000\",\n#     position = \"bottomright\",\n#     opacity = 0.7,\n#     labFormat = labelFormat(prefix = \"\"),\n#     labels = c(\"Low\", \"Medium-Low\", \"Medium\", \"Medium-High\", \"High\")\n#   )\n# \n# # Display the leaflet map\n# leaflet_map"
  },
  {
    "objectID": "posts/ggiraph/interactive graphics.html",
    "href": "posts/ggiraph/interactive graphics.html",
    "title": "ggiraph",
    "section": "",
    "text": "Show the code\nlibrary(ggiraph)\nlibrary(htmlwidgets)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(htmltools)\nlibrary(tidyr)\nlibrary(NHANES)\n\ndata(NHANES)\n\nNHANES_subset <- NHANES %>%\n  filter(!is.na(Diabetes), !is.na(PhysActive), !is.na(BMI), !is.na(Age)) %>%\n  mutate(Diabetes = ifelse(Diabetes == \"Yes\", \"Yes\", \"No\"),\n         PhysActive = ifelse(PhysActive == \"Yes\", \"Yes\", \"No\")) %>%\n  mutate(Diabetes = as.factor(Diabetes),\n         PhysActive = as.factor(PhysActive)) %>%\n    select(c(Diabetes, BMI, Age, PhysActive))\n\n\nggiraph\n\n\nShow the code\n# scatter_plot <- ggplot(NHANES_subset) +\n#   geom_point_interactive(\n#     aes(x = BMI, y = Age, color = Diabetes, shape = PhysActive, tooltip = paste(\"Diabetes:\", Diabetes, \"<br>Physically Active:\", PhysActive, \"<br>BMI:\", round(BMI, 1), \"<br>Age:\", Age)),\n#     size = 2,\n#     alpha = 0.6\n#   ) +\n#   scale_color_manual(values = c(\"#FF5964\", \"#35A7FF\")) +\n#   labs(\n#     title = \"Scatter Plot of Age vs BMI by Diabetes Status and Physical Activity\",\n#     x = \"BMI\",\n#     y = \"Age\",\n#     color = \"Diabetes Status\",\n#     shape = \"Physically Active\"\n#   ) +\n#   theme_minimal()\n# \n# # Save the plot as a png file\n# #ggsave(filename = \"interactive.png\", plot = scatter_plot, width = 10, height = 6, dpi = 300)\n# \n# scatter_plot_interactive <- girafe(code = print(scatter_plot), width_svg = 8, height_svg = 5)\n# scatter_plot_interactive\n\n\nSave scatterplot html\n\n\nShow the code\n#htmlwidgets::saveWidget(scatter_plot_interactive, \"interactive_scatter_plot.html\")\n\n\nDiabetes Prevalence and Mean BMI\n\n\nShow the code\n#added more variables\ncomplex_data_by_age <- NHANES %>%\n  mutate(AgeGroup = factor(case_when(\n    Age < 18 ~ \"Under 18\",\n    Age >= 18 & Age < 35 ~ \"18-34\",\n    Age >= 35 & Age < 50 ~ \"35-49\",\n    Age >= 50 ~ \"50+\"\n  ), levels = c(\"Under 18\", \"18-34\", \"35-49\", \"50+\"))) %>%\n  group_by(AgeGroup) %>%\n  summarize(\n    mean_bmi = mean(BMI, na.rm = TRUE),\n    mean_systolic_BP = mean(BPSysAve, na.rm = TRUE),\n    mean_diastolic_BP = mean(BPDiaAve, na.rm = TRUE),\n    diabetes_prevalence = mean(Diabetes == \"Yes\", na.rm = TRUE) * 100\n  )\n\n# Prepare the summary data for ggiraph\ncomplex_data_long <- complex_data_by_age %>%\n  gather(variable, value, -AgeGroup) %>%\n  mutate(label = round(value, 2))\n\n# Create the first ggplot object for Diabetes prevalence and Mean BMI\ngg1 <- ggplot(data = complex_data_long %>% filter(variable %in% c(\"diabetes_prevalence\", \"mean_bmi\")), \n              aes(x = AgeGroup, y = value, fill = variable, tooltip = label)) +\n  geom_bar_interactive(stat = \"identity\", position = \"dodge\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\", name = \"Health Metric\") +\n  labs(title = \"Diabetes Prevalence and Mean BMI by Age Group\", x = \"Age Group\", y = \"Value\") +\n  theme(legend.position = \"bottom\")\n\n# Create the second ggplot object for Mean Diastolic BP and Mean Systolic BP\ngg2 <- ggplot(data = complex_data_long %>% filter(variable %in% c(\"mean_diastolic_BP\", \"mean_systolic_BP\")), \n              aes(x = AgeGroup, y = value, fill = variable, tooltip = label)) +\n  geom_bar_interactive(stat = \"identity\", position = \"dodge\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\", name = \"Health Metric\") +\n  labs(title = \"Mean Diastolic and Systolic BP by Age Group\", x = \"Age Group\", y = \"Value\") +\n  theme(legend.position = \"bottom\")\n\n\nggiraph Vertically\n\n\nShow the code\n# Create the ggiraph objects with larger dimensions\ngiraph1 <- ggiraph(code = print(gg1), width_svg = 8, height_svg = 5)\ngiraph2 <- ggiraph(code = print(gg2), width_svg = 8, height_svg = 5)\n\n# Arrange the ggiraph objects vertically in a single column\nhtml_output <- tagList(\n  tags$head(tags$style(HTML(\"\n    .girafe-container {\n      width: 800px !important;\n      height: 500px !important;\n    }\n    .girafe-container > svg {\n      width: 100% !important;\n      height: 100% !important;\n    }\n  \"))),\n  tags$div(style = \"display:block;\", giraph1),\n  tags$div(style = \"display:block;\", giraph2)\n)\n\n# Save the output as an HTML file\nsave_html(html_output, \"vertical_ggiraph.html\")\n\n# Display the vertical ggiraph plots in the RStudio viewer or a web browser\nhtml_output"
  },
  {
    "objectID": "posts/Covid Project/Covid.html",
    "href": "posts/Covid Project/Covid.html",
    "title": "Covid Project",
    "section": "",
    "text": "Eat your Veggies!\nPrep and load libraries\n\n\nShow the code\nsetwd(\"/Users/isaacjohnson/Documents/Scanner Output/School/Willamette/Data 502 Data Visualization/Covid Healthy Diet/From Jesse/112622/DeBolt_Johnson_DATA502-Final\")\n\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(gapminder)\nlibrary(ggrepel)\nlibrary(RColorBrewer)\n\n\nReading in original data\n\n\nShow the code\nFood_Supply_kcal <- read.csv(\"/Users/isaacjohnson/Documents/Scanner Output/School/Willamette/Data 502 Data Visualization/Covid Healthy Diet/From Jesse/112622/DeBolt_Johnson_DATA502-Final/Food_Supply_kcal_Data.csv\", header = TRUE)\n\nkcal_continents_trimmed <- read.csv(\"/Users/isaacjohnson/Documents/Scanner Output/School/Willamette/Data 502 Data Visualization/Covid Healthy Diet/From Jesse/112622/DeBolt_Johnson_DATA502-Final/kcal_continent_trimmed.csv\", header = TRUE)\n\n\nCreating outlier groups\n\n\nShow the code\nusa <- kcal_continents_trimmed %>% \n  filter(Country == 'United States')\nslovakia <- kcal_continents_trimmed %>% \n  filter(Country == 'Slovakia')\nnigeria <- kcal_continents_trimmed %>% \n  filter(Country == 'Nigeria')\n\n\nCreating country groups\n\n\nShow the code\nEurope <- kcal_continents_trimmed %>% \n  filter(Continent == 'Europe')\nNorthAmerica <- kcal_continents_trimmed %>% \n  filter(Continent == 'North America')\nSouthAmerica <- kcal_continents_trimmed %>% \n  filter(Continent == 'South America')\nAfrica <- kcal_continents_trimmed %>% \n  filter(Continent == 'Africa')\n\nAmericas <- kcal_continents_trimmed %>% \n  filter(Continent == 'North America' | Continent == 'South America')\n\n\nAll with no labels\n\n\nShow the code\nggplot(kcal_continents_trimmed, aes(x=Vegetal.Products, y=Animal.fats,\n        size=Deaths, color=Continent, text=paste(Country)))+\n  scale_color_brewer(palette=\"Dark2\")+\n  geom_point(alpha=0.35, aes(size=(Deaths*1000)))+\n    scale_size_continuous(name = \"Deaths per 1000\")+\n    scale_x_continuous(limits = c(27, 48.5))+\n     #Quadrant lines\n  geom_vline(xintercept = mean(kcal_continents_trimmed$Vegetal.Products),\n             linetype=\"dotted\", size=1, color = \"grey\")+\n    geom_text(aes(mean(Vegetal.Products), 8.1, label = \"veggie consumption\",\n                  hjust=0), size=2.8, vjust=1.40, angle=270,color = \"grey\")+\n    geom_text(aes(mean(Vegetal.Products), 8.1, label = \"average\", \n                  hjust=0), size=2.8, vjust=-0.85, angle=270, color = \"grey\")+\n    geom_hline(yintercept = mean(kcal_continents_trimmed$Animal.fats),\n               linetype=\"dotted\", size=1, color = \"grey\")+\n    geom_text(aes(27, mean(Animal.fats), label = \"meat consumption\",\n                  vjust=1.15), size=2.8, hjust=0, color =\"grey\")+\n    geom_text(aes(27, mean(Animal.fats), label = \"average\",\n                  vjust=-0.65), size=2.8, hjust=0, color = \"grey\")+\n      \n  theme_minimal()+\n  \n  #Titles\n  ggtitle(\"Eat your VEGGIES kids!\")+\n    theme(plot.title = element_text(size=18))+\n    theme(plot.title = element_text(face=\"italic\"))+\n  labs(y=\"Animal Fats\",\n       x=\"Vegetable Products\")+\n    theme(axis.title.x = element_text(margin = margin(t=6, b=5), size=13))+\n    theme(axis.title.y = element_text(margin = margin(r=5, l=5), size=13))+\n\n  #Quadrant labels\n  annotate(\"text\", x=43, y=-0.5, label = \"Veggie based diet\", size=4,\n           fontface = \"italic\", hjust=0)+\n  annotate(\"text\", x=27.5, y=7.4, label = \"Meat based diet\", size=4,\n           fontface = \"italic\", hjust=0)\n\n\n\n\n\nEurope\n\n\nShow the code\nggplot(kcal_continents_trimmed, aes(x=Vegetal.Products, y=Animal.fats,\n        size=Deaths, text=paste(Country)))+\n  #scale_color_brewer(palette=\"Dark2\")+\n  geom_point(data = Europe, alpha=0.35, aes(size=(Deaths*1000)),\n             color = \"#7570B3\")+\n    scale_size_continuous(name = \"Deaths per 1000\")+\n    scale_x_continuous(limits = c(27, 48.5))+\n  \n  #Quadrant lines\n  geom_vline(xintercept = mean(kcal_continents_trimmed$Vegetal.Products),\n             linetype=\"dotted\", size=1, color = \"grey\")+\n    geom_text(aes(mean(Vegetal.Products), 8.1, label = \"veggie consumption\",\n                  hjust=0), size=2.8, vjust=1.40, angle=270,color = \"grey\")+\n    geom_text(aes(mean(Vegetal.Products), 8.1, label = \"average\", \n                  hjust=0), size=2.8, vjust=-0.85, angle=270, color = \"grey\")+\n    geom_hline(yintercept = mean(kcal_continents_trimmed$Animal.fats),\n               linetype=\"dotted\", size=1, color = \"grey\")+\n    geom_text(aes(27, mean(Animal.fats), label = \"meat consumption\",\n                  vjust=1.15), size=2.8, hjust=0, color =\"grey\")+\n    geom_text(aes(27, mean(Animal.fats), label = \"average\",\n                  vjust=-0.65), size=2.8, hjust=0, color = \"grey\")+\n  \n  theme_minimal()+\n  \n  #Titles\n  ggtitle(\"Eat your VEGGIES kids!\")+\n    theme(plot.title = element_text(size=18))+\n    theme(plot.title = element_text(face=\"italic\"))+\n  labs(y=\"Animal Fats\",\n       x=\"Vegetable Products\")+\n    theme(axis.title.x = element_text(margin = margin(t=6, b=5), size=13))+\n    theme(axis.title.y = element_text(margin = margin(r=5, l=5), size=13))+\n\n  #Quadrant labels\n  annotate(\"text\", x=43, y=-0.5, label = \"Veggie based diet\", size=4,\n           fontface = \"italic\", hjust=0)+\n  annotate(\"text\", x=27.5, y=7.4, label = \"Meat based diet\", size=4,\n           fontface = \"italic\", hjust=0)\n\n\n\n\n\nAmericas\n\n\nShow the code\nggplot(kcal_continents_trimmed, aes(x=Vegetal.Products, y=Animal.fats,\n        size=Deaths, text=paste(Country)))+\n  #scale_color_brewer(palette=\"Dark2\")+\n  geom_point(data = Americas, alpha=0.35, aes(size=(Deaths*1000)),\n             color = \"#E7298A\")+\n    scale_size_continuous(name = \"Deaths per 1000\")+\n    scale_x_continuous(limits = c(27, 48.5))+\n  \n  #Quadrant lines\n  geom_vline(xintercept = mean(kcal_continents_trimmed$Vegetal.Products),\n             linetype=\"dotted\", size=1, color = \"grey\")+\n    geom_text(aes(mean(Vegetal.Products), 8.1, label = \"veggie consumption\",\n                  hjust=0), size=2.8, vjust=1.40, angle=270,color = \"grey\")+\n    geom_text(aes(mean(Vegetal.Products), 8.1, label = \"average\", \n                  hjust=0), size=2.8, vjust=-0.85, angle=270, color = \"grey\")+\n    geom_hline(yintercept = mean(kcal_continents_trimmed$Animal.fats),\n               linetype=\"dotted\", size=1, color = \"grey\")+\n    geom_text(aes(27, mean(Animal.fats), label = \"meat consumption\",\n                  vjust=1.15), size=2.8, hjust=0, color =\"grey\")+\n    geom_text(aes(27, mean(Animal.fats), label = \"average\",\n                  vjust=-0.65), size=2.8, hjust=0, color = \"grey\")+\n  \n  theme_minimal()+\n  \n  #Titles\n  ggtitle(\"Eat your VEGGIES kids!\")+\n    theme(plot.title = element_text(size=18))+\n    theme(plot.title = element_text(face=\"italic\"))+\n  labs(y=\"Animal Fats\",\n       x=\"Vegetable Products\")+\n    theme(axis.title.x = element_text(margin = margin(t=6, b=5), size=13))+\n    theme(axis.title.y = element_text(margin = margin(r=5, l=5), size=13))+\n\n  #Quadrant labels\n  annotate(\"text\", x=43, y=-0.5, label = \"Veggie based diet\", size=4,\n           fontface = \"italic\", hjust=0)+\n  annotate(\"text\", x=27.5, y=7.4, label = \"Meat based diet\", size=4,\n           fontface = \"italic\", hjust=0)\n\n\n\n\n\nAfrica\n\n\nShow the code\nggplot(kcal_continents_trimmed, aes(x=Vegetal.Products, y=Animal.fats,\n        size=Deaths, text=paste(Country)))+\n  #scale_color_brewer(palette=\"Dark2\")+\n  geom_point(data = Africa, alpha=0.35, aes(size=(Deaths*1000)), \n             color = \"#1B9E77\")+\n    scale_size_continuous(name = \"Deaths per 1000\")+\n    scale_x_continuous(limits = c(27, 48.5))+\n  \n  #Quadrant lines\n  geom_vline(xintercept = mean(kcal_continents_trimmed$Vegetal.Products),\n             linetype=\"dotted\", size=1, color = \"grey\")+\n    geom_text(aes(mean(Vegetal.Products), 8.1, label = \"veggie consumption\",\n                  hjust=0), size=2.8, vjust=1.40, angle=270,color = \"grey\")+\n    geom_text(aes(mean(Vegetal.Products), 8.1, label = \"average\", \n                  hjust=0), size=2.8, vjust=-0.85, angle=270, color = \"grey\")+\n    geom_hline(yintercept = mean(kcal_continents_trimmed$Animal.fats),\n               linetype=\"dotted\", size=1, color = \"grey\")+\n    geom_text(aes(27, mean(Animal.fats), label = \"meat consumption\",\n                  vjust=1.15), size=2.8, hjust=0, color =\"grey\")+\n    geom_text(aes(27, mean(Animal.fats), label = \"average\",\n                  vjust=-0.65), size=2.8, hjust=0, color = \"grey\")+\n  \n  theme_minimal()+\n  \n  #Titles\n  ggtitle(\"Eat your VEGGIES kids!\")+\n    theme(plot.title = element_text(size=18))+\n    theme(plot.title = element_text(face=\"italic\"))+\n  labs(y=\"Animal Fats\",\n       x=\"Vegetable Products\")+\n    theme(axis.title.x = element_text(margin = margin(t=6, b=5), size=13))+\n    theme(axis.title.y = element_text(margin = margin(r=5, l=5), size=13))+\n\n  #Quadrant labels\n  annotate(\"text\", x=43, y=-0.5, label = \"Veggie based diet\", size=4,\n           fontface = \"italic\", hjust=0)+\n  annotate(\"text\", x=27.5, y=7.4, label = \"Meat based diet\", size=4,\n           fontface = \"italic\", hjust=0)\n\n\n\n\n\nAll with US highlighted\n\n\nShow the code\nggplot(kcal_continents_trimmed, aes(x=Vegetal.Products, y=Animal.fats,\n        size=Deaths, color=Continent, text=paste(Country)))+\n  scale_color_brewer(palette=\"Dark2\")+\n  geom_point(alpha=0.35, aes(size=(Deaths*1000)))+\n    scale_size_continuous(name = \"Deaths per 1000\")+\n    scale_x_continuous(limits = c(27, 48.5))+\n\n  #US and high/low outliers\n  geom_point(data = usa, aes(x=Vegetal.Products, y=Animal.fats),\n             color = \"#E7298A\", size=5)+\n    geom_label_repel(data = usa, aes(x=Vegetal.Products, y=Animal.fats,\n                    label = Country), fontface=\"bold\", size=4,\n                    nudge_x = -4.75, nudge_y = -1.0)+\n  # geom_point(data = slovakia, aes(x=Vegetal.Products, y=Animal.fats),\n  #            color = \"#7570B3\", size=4)+\n  #   geom_label_repel(data = slovakia, aes(x=Vegetal.Products, y=Animal.fats,\n  #                   label = Country), fontface=\"bold\", size=4,\n  #                   nudge_x = 2.75, nudge_y = -0.5)+\n  # geom_point(data = nigeria, aes(x=Vegetal.Products, y=Animal.fats),\n  #            color = \"#1B9E77\")+\n  #   geom_label_repel(data = nigeria, aes(x=Vegetal.Products, y=Animal.fats,\n  #                   label = Country), fontface=\"bold\", size=4, \n  #                   nudge_x = -1.5, nudge_y = 1.7)+\n  \n  #Quadrant lines\n  geom_vline(xintercept = mean(kcal_continents_trimmed$Vegetal.Products),\n             linetype=\"dotted\", size=1, color = \"grey\")+\n    geom_text(aes(mean(Vegetal.Products), 8.1, label = \"veggie consumption\",\n                  hjust=0), size=2.8, vjust=1.40, angle=270,color = \"grey\")+\n    geom_text(aes(mean(Vegetal.Products), 8.1, label = \"average\", \n                  hjust=0), size=2.8, vjust=-0.85, angle=270, color = \"grey\")+\n    geom_hline(yintercept = mean(kcal_continents_trimmed$Animal.fats),\n               linetype=\"dotted\", size=1, color = \"grey\")+\n    geom_text(aes(27, mean(Animal.fats), label = \"meat consumption\",\n                  vjust=1.15), size=2.8, hjust=0, color =\"grey\")+\n    geom_text(aes(27, mean(Animal.fats), label = \"average\",\n                  vjust=-0.65), size=2.8, hjust=0, color = \"grey\")+\n  \n  theme_minimal()+\n  \n  #Titles\n  ggtitle(\"Eat your VEGGIES kids!\")+\n    theme(plot.title = element_text(size=18))+\n    theme(plot.title = element_text(face=\"italic\"))+\n  labs(y=\"Animal Fats\",\n       x=\"Vegetable Products\")+\n    theme(axis.title.x = element_text(margin = margin(t=6, b=5), size=13))+\n    theme(axis.title.y = element_text(margin = margin(r=5, l=5), size=13))+\n\n  #Quadrant labels\n  annotate(\"text\", x=43, y=-0.5, label = \"Veggie based diet\", size=4,\n           fontface = \"italic\", hjust=0)+\n  annotate(\"text\", x=27.5, y=7.4, label = \"Meat based diet\", size=4,\n           fontface = \"italic\", hjust=0)\n\n\n\n\n\nAll with US and Slovakia highlighted\n\n\nShow the code\nggplot(kcal_continents_trimmed, aes(x=Vegetal.Products, y=Animal.fats,\n        size=Deaths, color=Continent, text=paste(Country)))+\n  scale_color_brewer(palette=\"Dark2\")+\n  geom_point(alpha=0.35, aes(size=(Deaths*1000)))+\n    scale_size_continuous(name = \"Deaths per 1000\")+\n    scale_x_continuous(limits = c(27, 48.5))+\n\n  #US and high/low outliers\n  geom_point(data = usa, aes(x=Vegetal.Products, y=Animal.fats),\n             color = \"#E7298A\", size=5)+\n    geom_label_repel(data = usa, aes(x=Vegetal.Products, y=Animal.fats,\n                    label = Country), fontface=\"bold\", size=4,\n                    nudge_x = -4.75, nudge_y = -1.0)+\n  geom_point(data = slovakia, aes(x=Vegetal.Products, y=Animal.fats),\n             color = \"#7570B3\", size=4)+\n    geom_label_repel(data = slovakia, aes(x=Vegetal.Products, y=Animal.fats,\n                    label = Country), fontface=\"bold\", size=4,\n                    nudge_x = 2.75, nudge_y = -0.5)+\n    \n  #Quadrant lines\n  geom_vline(xintercept = mean(kcal_continents_trimmed$Vegetal.Products),\n             linetype=\"dotted\", size=1, color = \"grey\")+\n    geom_text(aes(mean(Vegetal.Products), 8.1, label = \"veggie consumption\",\n                  hjust=0), size=2.8, vjust=1.40, angle=270,color = \"grey\")+\n    geom_text(aes(mean(Vegetal.Products), 8.1, label = \"average\", \n                  hjust=0), size=2.8, vjust=-0.85, angle=270, color = \"grey\")+\n    geom_hline(yintercept = mean(kcal_continents_trimmed$Animal.fats),\n               linetype=\"dotted\", size=1, color = \"grey\")+\n    geom_text(aes(27, mean(Animal.fats), label = \"meat consumption\",\n                  vjust=1.15), size=2.8, hjust=0, color =\"grey\")+\n    geom_text(aes(27, mean(Animal.fats), label = \"average\",\n                  vjust=-0.65), size=2.8, hjust=0, color = \"grey\")+\n  \n  theme_minimal()+\n  \n  #Titles\n  ggtitle(\"Eat your VEGGIES kids!\")+\n    theme(plot.title = element_text(size=18))+\n    theme(plot.title = element_text(face=\"italic\"))+\n  labs(y=\"Animal Fats\",\n       x=\"Vegetable Products\")+\n    theme(axis.title.x = element_text(margin = margin(t=6, b=5), size=13))+\n    theme(axis.title.y = element_text(margin = margin(r=5, l=5), size=13))+\n\n  #Quadrant labels\n  annotate(\"text\", x=43, y=-0.5, label = \"Veggie based diet\", size=4,\n           fontface = \"italic\", hjust=0)+\n  annotate(\"text\", x=27.5, y=7.4, label = \"Meat based diet\", size=4,\n           fontface = \"italic\", hjust=0)\n\n\n\n\n\nAll with US, Slovakia, and Nigeria highlighted\n\n\nShow the code\nggplot(kcal_continents_trimmed, aes(x=Vegetal.Products, y=Animal.fats,\n        size=Deaths, color=Continent, text=paste(Country)))+\n  scale_color_brewer(palette=\"Dark2\")+\n  geom_point(alpha=0.35, aes(size=(Deaths*1000)))+\n    scale_size_continuous(name = \"Deaths per 1000\")+\n    scale_x_continuous(limits = c(27, 48.5))+\n\n  #US and high/low outliers\n  geom_point(data = usa, aes(x=Vegetal.Products, y=Animal.fats),\n             color = \"#E7298A\", size=5)+\n    geom_label_repel(data = usa, aes(x=Vegetal.Products, y=Animal.fats,\n                    label = Country), fontface=\"bold\", size=4,\n                    nudge_x = -4.75, nudge_y = -1.0)+\n  geom_point(data = slovakia, aes(x=Vegetal.Products, y=Animal.fats),\n             color = \"#7570B3\", size=4)+\n    geom_label_repel(data = slovakia, aes(x=Vegetal.Products, y=Animal.fats,\n                    label = Country), fontface=\"bold\", size=4,\n                    nudge_x = 2.75, nudge_y = -0.5)+\n  geom_point(data = nigeria, aes(x=Vegetal.Products, y=Animal.fats),\n             color = \"#1B9E77\")+\n    geom_label_repel(data = nigeria, aes(x=Vegetal.Products, y=Animal.fats,\n                    label = Country), fontface=\"bold\", size=4,\n                    nudge_x = -1.5, nudge_y = 1.7)+\n  \n  #Quadrant lines\n  geom_vline(xintercept = mean(kcal_continents_trimmed$Vegetal.Products),\n             linetype=\"dotted\", size=1, color = \"grey\")+\n    geom_text(aes(mean(Vegetal.Products), 8.1, label = \"veggie consumption\",\n                  hjust=0), size=2.8, vjust=1.40, angle=270,color = \"grey\")+\n    geom_text(aes(mean(Vegetal.Products), 8.1, label = \"average\", \n                  hjust=0), size=2.8, vjust=-0.85, angle=270, color = \"grey\")+\n    geom_hline(yintercept = mean(kcal_continents_trimmed$Animal.fats),\n               linetype=\"dotted\", size=1, color = \"grey\")+\n    geom_text(aes(27, mean(Animal.fats), label = \"meat consumption\",\n                  vjust=1.15), size=2.8, hjust=0, color =\"grey\")+\n    geom_text(aes(27, mean(Animal.fats), label = \"average\",\n                  vjust=-0.65), size=2.8, hjust=0, color = \"grey\")+\n  \n  theme_minimal()+\n  \n  #Titles\n  ggtitle(\"Eat your VEGGIES kids!\")+\n    theme(plot.title = element_text(size=18))+\n    theme(plot.title = element_text(face=\"italic\"))+\n  labs(y=\"Animal Fats\",\n       x=\"Vegetable Products\")+\n    theme(axis.title.x = element_text(margin = margin(t=6, b=5), size=13))+\n    theme(axis.title.y = element_text(margin = margin(r=5, l=5), size=13))+\n\n  #Quadrant labels\n  annotate(\"text\", x=43, y=-0.5, label = \"Veggie based diet\", size=4,\n           fontface = \"italic\", hjust=0)+\n  annotate(\"text\", x=27.5, y=7.4, label = \"Meat based diet\", size=4,\n           fontface = \"italic\", hjust=0)\n\n\n\n\n\nEverything\n\n\nShow the code\nggplot(kcal_continents_trimmed, aes(x=Vegetal.Products, y=Animal.fats,\n        size=Deaths, color=Continent, text=paste(Country)))+\n  scale_color_brewer(palette=\"Dark2\")+\n  geom_point(alpha=0.35, aes(size=(Deaths*1000)))+\n    scale_size_continuous(name = \"Deaths per 1000\")+\n    scale_x_continuous(limits = c(27, 48.5))+\n  \n  #US and high/low outliers\n  geom_point(data = usa, aes(x=Vegetal.Products, y=Animal.fats),\n             color = \"#E7298A\", size=5)+\n    geom_label_repel(data = usa, aes(x=Vegetal.Products, y=Animal.fats,\n                    label = Country), fontface=\"bold\", size=4,\n                    nudge_x = -4.75, nudge_y = -1.0)+\n  geom_point(data = slovakia, aes(x=Vegetal.Products, y=Animal.fats),\n             color = \"#7570B3\", size=4)+\n    geom_label_repel(data = slovakia, aes(x=Vegetal.Products, y=Animal.fats,\n                    label = Country), fontface=\"bold\", size=4,\n                    nudge_x = 2.75, nudge_y = -0.5)+\n  geom_point(data = nigeria, aes(x=Vegetal.Products, y=Animal.fats),\n             color = \"#1B9E77\")+\n    geom_label_repel(data = nigeria, aes(x=Vegetal.Products, y=Animal.fats,\n                    label = Country), fontface=\"bold\", size=4,\n                    nudge_x = -1.5, nudge_y = 1.7)+\n  \n  #Quadrant lines\n  geom_vline(xintercept = mean(kcal_continents_trimmed$Vegetal.Products),\n             linetype=\"dotted\", size=1, color = \"grey\")+\n    geom_text(aes(mean(Vegetal.Products), 8.1, label = \"veggie consumption\",\n                  hjust=0), size=2.8, vjust=1.40, angle=270,color = \"grey\")+\n    geom_text(aes(mean(Vegetal.Products), 8.1, label = \"average\", \n                  hjust=0), size=2.8, vjust=-0.85, angle=270, color = \"grey\")+\n    geom_hline(yintercept = mean(kcal_continents_trimmed$Animal.fats),\n               linetype=\"dotted\", size=1, color = \"grey\")+\n    geom_text(aes(27, mean(Animal.fats), label = \"meat consumption\",\n                  vjust=1.15), size=2.8, hjust=0, color =\"grey\")+\n    geom_text(aes(27, mean(Animal.fats), label = \"average\",\n                  vjust=-0.65), size=2.8, hjust=0, color = \"grey\")+\n  \n  theme_minimal()+\n  \n  #Titles\n  ggtitle(\"Eat your VEGGIES kids!\")+\n    theme(plot.title = element_text(size=18))+\n    theme(plot.title = element_text(face=\"italic\"))+\n  labs(y=\"Animal Fats\",\n       x=\"Vegetable Products\")+\n    theme(axis.title.x = element_text(margin = margin(t=6, b=5), size=13))+\n    theme(axis.title.y = element_text(margin = margin(r=5, l=5), size=13))+\n\n  #Quadrant labels\n  annotate(\"text\", x=43, y=-0.5, label = \"Veggie based diet\", size=4,\n           fontface = \"italic\", hjust=0)+\n  annotate(\"text\", x=27.5, y=7.4, label = \"Meat based diet\", size=4,\n           fontface = \"italic\", hjust=0)\n\n\n\n\n\nPlotly finale\n\n\nShow the code\n# veggie_plotly <- ggplot(kcal_continents_trimmed, aes(x=Vegetal.Products, y=Animal.fats,\n#         size=Deaths, color=Continent, text=paste(Country)))+\n#   scale_color_brewer(palette=\"Dark2\")+\n#   geom_point(alpha=0.35, aes(size=(Deaths*1000)))+\n#     scale_size_continuous(name = \"Deaths per 1000\")+\n#     scale_x_continuous(limits = c(27, 48.5))+\n#   \n#   #US and high/low outliers\n#   geom_point(data = usa, aes(x=Vegetal.Products, y=Animal.fats),\n#              color = \"#E7298A\", size=5)+\n#     geom_label_repel(data = usa, aes(x=Vegetal.Products, y=Animal.fats,\n#                     label = Country), fontface=\"bold\", size=4,\n#                     nudge_x = -4.75, nudge_y = -1.0)+\n#   geom_point(data = slovakia, aes(x=Vegetal.Products, y=Animal.fats),\n#              color = \"#7570B3\", size=4)+\n#     geom_label_repel(data = slovakia, aes(x=Vegetal.Products, y=Animal.fats,\n#                     label = Country), fontface=\"bold\", size=4,\n#                     nudge_x = 2.75, nudge_y = -0.5)+\n#   geom_point(data = nigeria, aes(x=Vegetal.Products, y=Animal.fats),\n#              color = \"#1B9E77\")+\n#     geom_label_repel(data = nigeria, aes(x=Vegetal.Products, y=Animal.fats,\n#                     label = Country), fontface=\"bold\", size=4,\n#                     nudge_x = -1.5, nudge_y = 1.7)+\n#   \n#   #Quadrant lines\n#   geom_vline(xintercept = mean(kcal_continents_trimmed$Vegetal.Products),\n#              linetype=\"dotted\", size=1, color = \"grey\")+\n#     geom_hline(yintercept = mean(kcal_continents_trimmed$Animal.fats),\n#                linetype=\"dotted\", size=1, color = \"grey\")+\n#   \n#   theme_minimal()+\n#   \n#   #Titles\n#   ggtitle(\"Eat your VEGGIES kids!\")+\n#     theme(plot.title = element_text(size=18))+\n#     theme(plot.title = element_text(face=\"italic\"))+\n#   labs(y=\"Animal Fats\",\n#        x=\"Vegetable Products\")+\n#     theme(axis.title.x = element_text(margin = margin(t=6, b=5), size=13))+\n#     theme(axis.title.y = element_text(margin = margin(r=5, l=5), size=13))\n# \n# ggplotly(veggie_plotly, tooltip = \"text\")\n\n\nSave plot\n\n\nShow the code\nggsave(\"covid.png\", width = 9, height = 7, units = \"in\")"
  },
  {
    "objectID": "posts/ducks_tidy_tuesday/index.html",
    "href": "posts/ducks_tidy_tuesday/index.html",
    "title": "#TidyTuesday inspired by @osteobjorn | #TidyTuesday 06-10-2020",
    "section": "",
    "text": "Show the code\n#| echo: true\n\nlibrary(ggplot2)\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(cowplot)\nlibrary(magick)\n\n\ndata <- tt_load('2020-10-06')\n\n\n\n    Downloading file 1 of 1: `tournament.csv`\n\n\nShow the code\ntournament <- data$tournament \n\noregon <- tournament %>%\n  filter(school == \"Oregon\") %>%\n  select(school, year, full_percent, tourney_finish)\n\n#save locally\nwrite.csv(tournament, \"/Users/isaacjohnson/Documents/Scanner Output/School/Willamette/Communicating with Data 020123/tournament.csv\", row.names=FALSE)"
  },
  {
    "objectID": "posts/ducks_tidy_tuesday/index.html#plot-the-data",
    "href": "posts/ducks_tidy_tuesday/index.html#plot-the-data",
    "title": "#TidyTuesday inspired by @osteobjorn | #TidyTuesday 06-10-2020",
    "section": "Plot the data",
    "text": "Plot the data\n\n\nShow the code\nggplot(oregon, aes(x = year, y = full_percent)) +\n    geom_path() +\n    geom_point() +\n    geom_text(aes(label = paste0(tourney_finish)))"
  },
  {
    "objectID": "posts/ducks_tidy_tuesday/index.html#clean-up-the-chart",
    "href": "posts/ducks_tidy_tuesday/index.html#clean-up-the-chart",
    "title": "#TidyTuesday inspired by @osteobjorn | #TidyTuesday 06-10-2020",
    "section": "Clean up the chart",
    "text": "Clean up the chart\n\n\nShow the code\nggplot(oregon, aes(x = year, y = full_percent)) + \n  geom_path() + \n  geom_point() + \n  geom_text(aes(label = paste0(tourney_finish)), \n            nudge_y = c(2, 2, -2, 2, -1, -2, 2, -2, 2, 0, -2, 1.5, -2, .5),\n            nudge_x = c(0, 0, 0, 0, -1, 0, 0, 0, 0, 1, 0, 0.5, 0, -1))"
  },
  {
    "objectID": "posts/ducks_tidy_tuesday/index.html#more-editing-the-plot",
    "href": "posts/ducks_tidy_tuesday/index.html#more-editing-the-plot",
    "title": "#TidyTuesday inspired by @osteobjorn | #TidyTuesday 06-10-2020",
    "section": "More editing the plot",
    "text": "More editing the plot\n\n\nShow the code\nggplot(oregon, aes(x = year, y = full_percent)) + \n  geom_path() + \n  geom_point() + \n  geom_text(aes(label = paste0(tourney_finish)), \n            nudge_y = c(2, 2, -2, 2, -1, -2, 2, -2, 2, 0, -2, 1.5, -2, .5),\n            nudge_x = c(0, 0, 0, 0, -1, 0, 0, 0, 0, 1, 0, 0.5, 0, -1)) + \n  labs(title = \"Oregon NCAA Tournament History\",\n       subtitle = \"Total win/loss percentage and end result by year\",\n      x = \"Year\", y = \"Total win/loss percentage\", \n      caption = \"inspired by @osteobjorn | #TidyTuesday 06-10-2020 | source: FiveThirtyEight\")"
  },
  {
    "objectID": "posts/ducks_tidy_tuesday/index.html#add-color",
    "href": "posts/ducks_tidy_tuesday/index.html#add-color",
    "title": "#TidyTuesday inspired by @osteobjorn | #TidyTuesday 06-10-2020",
    "section": "Add color",
    "text": "Add color\n\n\nShow the code\nggplot(oregon, aes(x = year, y = full_percent)) + \n  geom_path(col = \"#FEE123\") + \n  geom_point(col = \"#FEE123\") + \n  geom_text(aes(label = paste0(tourney_finish)), \n            col = \"white\",\n            nudge_y = c(2, 2, -2, 2, -1, -2, 2, -2, 2, 0, -2, 1.5, -2, .5),\n            nudge_x = c(0, 0, 0, 0, -1, 0, 0, 0, 0, 1, 0, 0.5, 0, -1)) + \n  labs(title = \"Oregon NCAA Tournament History\",\n       subtitle = \"Total win/loss percentage and end result by year\",\n      x = \"Year\", y = \"Total win/loss percentage\", \n      caption = \"inspired by @osteobjorn | #TidyTuesday 06-10-2020 | source: FiveThirtyEight\")+\n    theme(plot.background = element_rect(fill = \"#154733\"), # background colour\n        plot.title = element_text(colour = \"#FEE123\", # text colour\n                                  size = 20, # font size\n                                  face = \"bold\"), # bold text\n        plot.subtitle = element_text(colour = \"#FEE123\",\n                                     size = 14),\n        plot.caption = element_text(colour = \"#FEE123\"),\n        axis.text = element_text(colour = \"#FEE123\"),\n        axis.title.x = element_blank(), \n        axis.title.y = element_blank(), \n        axis.text.y = element_blank(),\n        plot.margin = unit(c(1,1,1,1), \"cm\"), \n        panel.background = element_rect(fill = \"#154733\"), \n        panel.grid = element_blank())"
  },
  {
    "objectID": "posts/ducks_tidy_tuesday/index.html#added-logo",
    "href": "posts/ducks_tidy_tuesday/index.html#added-logo",
    "title": "#TidyTuesday inspired by @osteobjorn | #TidyTuesday 06-10-2020",
    "section": "Added logo",
    "text": "Added logo\n\n\nShow the code\nimg <- image_read(path = \"https://goducks.com/images/uoregon_logo.png\")\n\np <- ggplot(oregon, aes(x = year, y = full_percent)) + \n  geom_path(col = \"#FEE123\") + \n  geom_point(col = \"#FEE123\") + \n  geom_text(aes(label = paste0(tourney_finish)), \n            col = \"white\",\n            nudge_y = c(2, 2, -2, 2, -1, -2, 2, -2, 2, 0, -2, 1.5, -2, .5),\n            nudge_x = c(0, 0, 0, 0, -1, 0, 0, 0, 0, 1, 0, 0.5, 0, -1)) + \n  labs(title = \"Oregon NCAA Tournament History\",\n       subtitle = \"Total win/loss percentage and end result by year\",\n      x = \"Year\", y = \"Total win/loss percentage\", \n      caption = \"inspired by @osteobjorn | #TidyTuesday 06-10-2020 | source: FiveThirtyEight\")+\n    theme(plot.background = element_rect(fill = \"#154733\"), # background colour\n        plot.title = element_text(colour = \"#FEE123\", # text colour\n                                  size = 20, # font size\n                                  face = \"bold\"), # bold text\n        plot.subtitle = element_text(colour = \"#FEE123\",\n                                     size = 14),\n        plot.caption = element_text(colour = \"#FEE123\"),\n        axis.text = element_text(colour = \"#FEE123\"),\n        axis.title.x = element_blank(), \n        axis.title.y = element_blank(), \n        axis.text.y = element_blank(),\n        plot.margin = unit(c(1,1,1,1), \"cm\"), \n        panel.background = element_rect(fill = \"#154733\"), \n        panel.grid = element_blank()) \n\n\nggdraw() +\n  draw_plot(p) + \n  draw_image(img, scale = 0.2, hjust = 0.4, vjust = 0.2)\n\n\n\n\n\n\n\nShow the code\nggsave(\"oregon.png\", width = 9, height = 7, units = \"in\")"
  },
  {
    "objectID": "posts/presentation/presentation.html#why-data-science",
    "href": "posts/presentation/presentation.html#why-data-science",
    "title": "Data Science Portfolio",
    "section": "Why Data Science?",
    "text": "Why Data Science?\n\nI am passionate about using the data effectively\nWant to do something that I am interested in\nI have always been a logical thinker\n\nhave excelled at math and computers\n\nHave ability to effectively communicate complex ideas simply"
  },
  {
    "objectID": "posts/presentation/presentation.html#what-am-i-learning",
    "href": "posts/presentation/presentation.html#what-am-i-learning",
    "title": "Data Science Portfolio",
    "section": "What am I learning?",
    "text": "What am I learning?\n\nI have taken classes in:\n\nFoundations of Data Science in R\nData Visualization in R\nData Engineering in MySQL"
  },
  {
    "objectID": "posts/presentation/presentation.html#what-am-i-learning-1",
    "href": "posts/presentation/presentation.html#what-am-i-learning-1",
    "title": "Data Science Portfolio",
    "section": "What am I learning?",
    "text": "What am I learning?\n\nI have taken classes in:\n\nCommunicating with Data\nMachine Learning in R\nSurvival Analysis in R"
  },
  {
    "objectID": "posts/presentation/presentation.html#what-will-i-learn-next",
    "href": "posts/presentation/presentation.html#what-will-i-learn-next",
    "title": "Data Science Portfolio",
    "section": "What will I learn next",
    "text": "What will I learn next\n\nData Ethics\nPython\nMaster’s Capstone project\n\nWill graduate August 2023!"
  },
  {
    "objectID": "posts/presentation/presentation.html#work-examples",
    "href": "posts/presentation/presentation.html#work-examples",
    "title": "Data Science Portfolio",
    "section": "Work Examples",
    "text": "Work Examples"
  },
  {
    "objectID": "posts/presentation/presentation.html#interactive-visualizations-with-leaflet",
    "href": "posts/presentation/presentation.html#interactive-visualizations-with-leaflet",
    "title": "Data Science Portfolio",
    "section": "Interactive Visualizations with Leaflet",
    "text": "Interactive Visualizations with Leaflet"
  },
  {
    "objectID": "posts/presentation/presentation.html#interactive-visualizations-with-ggiraph",
    "href": "posts/presentation/presentation.html#interactive-visualizations-with-ggiraph",
    "title": "Data Science Portfolio",
    "section": "Interactive Visualizations with ggiraph",
    "text": "Interactive Visualizations with ggiraph"
  },
  {
    "objectID": "posts/presentation/presentation.html#interactive-visualizations-with-dt-table",
    "href": "posts/presentation/presentation.html#interactive-visualizations-with-dt-table",
    "title": "Data Science Portfolio",
    "section": "Interactive Visualizations with DT table",
    "text": "Interactive Visualizations with DT table"
  },
  {
    "objectID": "posts/presentation/presentation.html#interactive-visualizations-with-plotly",
    "href": "posts/presentation/presentation.html#interactive-visualizations-with-plotly",
    "title": "Data Science Portfolio",
    "section": "Interactive Visualizations with plotly",
    "text": "Interactive Visualizations with plotly"
  },
  {
    "objectID": "posts/presentation/presentation.html#thank-you",
    "href": "posts/presentation/presentation.html#thank-you",
    "title": "Data Science Portfolio",
    "section": "Thank you!",
    "text": "Thank you!\n\nFor more to come, please check out my wesbite\nPortfolio website: https://isaacajohnson.github.io/Portfolio/"
  },
  {
    "objectID": "posts/Python Group Project/index.html#introduction",
    "href": "posts/Python Group Project/index.html#introduction",
    "title": "Healthcare Access and its Effects on Coronary Heart Disease Prevalence",
    "section": "Introduction:",
    "text": "Introduction:\nThis project aims to leverage machine learning techniques to predict the occurrence of Coronary Heart Disease (CHD), a leading cause of death globally. We believe that early prediction of CHD can lead to preventative measures, better health outcomes, and potentially save lives. Our approach involves using health and demographic data to train predictive models, specifically utilizing Support Vector Machines (SVM), Random Forests, and k-means clustering algorithms. The models are trained and evaluated using a dataset from the CDC, which includes information such as age, gender, cholesterol levels, and other health indicators."
  },
  {
    "objectID": "posts/Python Group Project/index.html#problem-statement",
    "href": "posts/Python Group Project/index.html#problem-statement",
    "title": "Healthcare Access and its Effects on Coronary Heart Disease Prevalence",
    "section": "Problem Statement:",
    "text": "Problem Statement:\nCoronary Heart Disease is a leading cause of death globally. Early prediction of the disease can lead to early intervention and potentially save lives. The challenge is to build an accurate and reliable predictive model using available health and demographic data."
  },
  {
    "objectID": "posts/Python Group Project/index.html#research-questions",
    "href": "posts/Python Group Project/index.html#research-questions",
    "title": "Healthcare Access and its Effects on Coronary Heart Disease Prevalence",
    "section": "Research Questions:",
    "text": "Research Questions:\n\nCan we predict the presence of Coronary Heart Disease using health and demographic data?\nHow we’ll do SVM, Random Forest, and k-means clustering algorithms perform in predicting CHD?\nWhich features are most predictive of CHD according to our models?"
  },
  {
    "objectID": "posts/Python Group Project/index.html#methodology",
    "href": "posts/Python Group Project/index.html#methodology",
    "title": "Healthcare Access and its Effects on Coronary Heart Disease Prevalence",
    "section": "Methodology:",
    "text": "Methodology:\nOur methodology involves several key steps. First, we preprocessed the dataset obtained from the CDC, which involved handling missing values, one-hot encoding categorical variables, and scaling numerical variables.\n\n\nShow the code\n#Import needed libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n#Importing data\ncdc = pd.read_csv('data/CDC_for_python.csv')\ncdc = cdc.loc[cdc.CHD > 0]\n\n#View data structure\ncdc.shape\n\n#### Data cleaning ####\ncdc = cdc[~cdc['display_name'].str.contains(r'\\(AS\\)|\\(GU\\)|\\(MP\\)|\\(PR\\)|\\(County Equivalent\\)')]\n\n# Separate the 'display_name' column\ncdc[['county', 'state']] = cdc['display_name'].str.extract(r'\\\"(.+), \\((.+)\\)\\\"', expand=True)\n\n# Remove the original column\ncdc = cdc.drop(['display_name'], axis=1)\n\n#change rural/urban\n# 1 = Large central metro -> Large_Urban\n# 2 = Large fringe metro -> LargeFringe_Urban\n# 3 = Medium/small metro -> MediumSmall_Urban\n# 4 = Nonmetro -> Rural\n\n# Replace values in 'UrbanRural' column\ncdc['UrbanRural'] = cdc['UrbanRural'].replace({1: 'Large_Urban', 2: 'LargeFringe_Urban', 3: 'MediumSmall_Urban', 4: 'Rural'})\n\n# Replace -1 with NaN\ncdc = cdc.replace(-1, np.nan)\n\n# Display column names\nprint(cdc.columns)\n\n# Show unique values in 'UrbanRural'\nprint(cdc['UrbanRural'].unique())\n\n# Create a subset where 'UrbanRural' is NaN, an empty string, or 'NA'\nrural_query = cdc[cdc['UrbanRural'].isin([pd.np.nan, '', 'NA'])]\n\n# Display the first 20 rows of this subset\nrural_query.head(20)\n\n# Find and insert where county is 'Kusilvak' based on Wikipedia data.\ncdc.loc[cdc['county'].str.contains('Kusilvak', na=False), 'UrbanRural'] = \"Rural\"\n\n# Display unique values in the 'UrbanRural' column\nprint(cdc['UrbanRural'].unique())\n\n# Double check for any NAs in Rural/Urban\nrural_query = cdc[cdc['UrbanRural'].isin([None, \"\", \"NA\"])].head(20)\nrural_query\n\n# Convert fips to string\ncdc['fips'] = cdc['fips'].astype(str)\n\n# Inserting Parks missing value\ncdc.loc[cdc['county'].str.contains('Kusilvak', na=False), 'Parks'] = 66\n\n# Total number of missing values in dataset\ntotal_na = cdc.isnull().sum().sum()\nprint(total_na)\n\n# Total number of missing values in each column\ncolumn_na = cdc.isnull().sum()\nprint(column_na)\n\n# Total number of missing values in each row\ncdc['count_na'] = cdc.isnull().sum(axis=1)\n\n# Sort by 'count_na'\ncdc = cdc.sort_values(by='count_na', ascending=False)\n\n# Remove those that have more than 8 NAs in that row\ncdc = cdc[cdc['count_na'] <= 8]\n\n# Remove 'count_na' column\ncdc = cdc.drop('count_na', axis=1)\n\n## Inserting values for missing data for NJ\n# bpmUse\ncdc.loc[cdc['state'].str.contains('NJ', na=False), 'bpmUse'] = 71.71\n\n# CholScreen\ncdc.loc[cdc['state'].str.contains('NJ', na=False), 'CholScreen'] = 79.43\n\n# HighBP\ncdc.loc[cdc['state'].str.contains('NJ', na=False), 'HighBP'] = 33.7\n\n# Diabetes\ncdc.loc[cdc['state'].str.contains('NJ', na=False), 'Diabetes'] = 17.4\n\n# HighChol\ncdc.loc[cdc['state'].str.contains('NJ', na=False), 'HighChol'] = 32.41\n\n# Obesity\ncdc.loc[cdc['state'].str.contains('NJ', na=False), 'Obesity'] = 33.59\n\n## Inserting values for missing values in Median Home Value\ncdc.loc[cdc['fips'].str.contains('48261', na=False), 'MedHomeValue'] = 42550\ncdc.loc[cdc['fips'].str.contains('48301', na=False), 'MedHomeValue'] = 38143\ncdc.loc[cdc['fips'].str.contains('46017', na=False), 'MedHomeValue'] = 101393\ncdc.loc[cdc['fips'].str.contains('46095', na=False), 'MedHomeValue'] = 60537\n\n## Inserting missing values for PCP and Cardio Phys\n# Read in dataset with values for merging\npcp_cardio_count = pd.read_csv(\"data/pcp_cardio_count.csv\")\n\n# Convert the 'COUNTY' column to string\npcp_cardio_count['COUNTY'] = pcp_cardio_count['COUNTY'].astype(str)\n\n# Joining count data frame to the cdc data frame\ncdc = cdc.merge(pcp_cardio_count, left_on='fips', right_on='COUNTY', how='left')\n\n# Use fillna to replace NAs in PCP and CardioPhys\ncdc['pcp'] = cdc['pcp'].fillna(cdc['PrimaryCarePhys'])\ncdc['CardioPhys'] = cdc['CardioPhys'].fillna(cdc['cardio'])\n\n# Remove the temporary columns\ncdc = cdc.drop(['PrimaryCarePhys', 'cardio', 'COUNTY'], axis=1)\n\n# Check DataFrame for missing or NA values, but only display columns with missing values\nmissing_values = cdc.isnull().sum()\nmissing_values = missing_values[missing_values != 0]\nprint(missing_values)\n\n#### Data Engineering ####\n#Impute for missing values\nfrom sklearn.impute import SimpleImputer\n\n# Specify the columns you want to impute\ncolumns_to_impute = ['CholMedNonAdhear', 'CholMedElegible', 'cruParticipate', 'CardioPhys', 'PhysInactivity', 'AirQuality', 'pcp']\n\n# Subset the DataFrame to only these columns\nsubset_cdc = cdc[columns_to_impute]\n\n# Create an imputer object\nimputer = SimpleImputer(strategy='mean')\n\n# Fit the imputer to the data and transform the data\nimputed_data = imputer.fit_transform(subset_cdc)\n\n# Convert the result back to a DataFrame\nimputed_data = pd.DataFrame(imputed_data, columns=subset_cdc.columns)\n\n# Replace the original columns in the DataFrame with the imputed data\ncdc[columns_to_impute] = imputed_data\n\ncdc['IsRural'] = cdc['UrbanRural'] == 'Rural'\n\n# Checking status of new column\nprint(cdc.IsRural.unique())\nprint(cdc['IsRural'].dtype)\n\n#Create classes for CHD prevalence\n# Define the boundaries for the quantiles\nquantiles = cdc['CHD'].quantile([0.33, 0.66]).values\n\n# Function to classify 'CHD' values\ndef classify_chd(value):\n    if value <= quantiles[0]:\n        return 'Low'\n    elif value <= quantiles[1]:\n        return 'Medium'\n    else:\n        return 'High'\n\n# Add 'CHD_Class' column\ncdc['CHD_Class'] = cdc['CHD'].apply(classify_chd)\n\n# Print the dataframe\nprint(cdc)\n\n# Define the regions\nregions = {\n    'Northeast': ['CT', 'ME', 'MA', 'NH', 'RI', 'VT', 'NJ', 'NY', 'PA'],\n    'Midwest': ['IL', 'IN', 'MI', 'OH', 'WI', 'IA', 'KS', 'MN', 'MO', 'NE', 'ND', 'SD'],\n    'South': ['DE', 'FL', 'GA', 'MD', 'NC', 'SC', 'VA', 'DC', 'WV', 'AL', 'KY', 'MS', 'TN', 'AR', 'LA', 'OK', 'TX'],\n    'West': ['AZ', 'CO', 'ID', 'MT', 'NV', 'NM', 'UT', 'WY', 'AK', 'CA', 'HI', 'OR', 'WA']\n}\n\n# Function to assign region based on state\ndef assign_region(state):\n    for region, states in regions.items():\n        if state in states:\n            return region\n    return 'Other'\n\n# Add 'region' column\ncdc['region'] = cdc['state'].apply(assign_region)\n\n# Print the updated DataFrame\nprint(cdc)\n\n\nIndex(['fips', 'Age65Plus', 'AIAN', 'AIANmen', 'AIANwomen', 'ANHPI',\n       'ANHPImen', 'ANHPIwomen', 'Black', 'BlackMen', 'BlackWomen', 'Hispanic',\n       'HispanicMen', 'HispanicWomen', 'OtherRace', 'OtherRaceMen',\n       'OtherRaceWomen', 'PopAllGenders', 'PopMen', 'PopWomen', 'pop',\n       'TwoPlus', 'TwoPlusMen', 'TwoPlusWomen', 'White', 'WhiteMen',\n       'WhiteWomen', 'bpmUse', 'CholScreen', 'CholMedNonAdhear',\n       'CholMedElegible', 'cruParticipate', 'Hospitals', 'HospCIC', 'HospCR',\n       'HospED', 'Pharmacies', 'HealthIns', 'CardioPhys', 'PrimaryCarePhys',\n       'CHD', 'HighBP', 'Stroke', 'Diabetes', 'HighChol', 'Obesity',\n       'PhysInactivity', 'Smoker', 'AirQuality', 'Parks', 'Broadband',\n       'EdLessColl', 'SNAPrecipients', 'MedHomeValue', 'MedHouseIncome',\n       'Poverty', 'Unemploy', 'UrbanRural', 'county', 'state'],\n      dtype='object')\n['Rural' 'MediumSmall_Urban' nan 'Large_Urban' 'LargeFringe_Urban']\n['Rural' 'MediumSmall_Urban' 'Large_Urban' 'LargeFringe_Urban']\n3297\nfips                   0\nAge65Plus              0\nAIAN                   0\nAIANmen                0\nAIANwomen              0\nANHPI                  0\nANHPImen               0\nANHPIwomen             0\nBlack                  0\nBlackMen               0\nBlackWomen             0\nHispanic               0\nHispanicMen            0\nHispanicWomen          0\nOtherRace              0\nOtherRaceMen           0\nOtherRaceWomen         0\nPopAllGenders          0\nPopMen                 0\nPopWomen               0\npop                    0\nTwoPlus                0\nTwoPlusMen             0\nTwoPlusWomen           0\nWhite                  0\nWhiteMen               0\nWhiteWomen             0\nbpmUse                21\nCholScreen            21\nCholMedNonAdhear      81\nCholMedElegible      111\ncruParticipate       743\nHospitals              0\nHospCIC                0\nHospCR                 0\nHospED                 0\nPharmacies             0\nHealthIns              1\nCardioPhys          1965\nPrimaryCarePhys      220\nCHD                    0\nHighBP                21\nStroke                 0\nDiabetes              21\nHighChol              21\nObesity               21\nPhysInactivity        21\nSmoker                 0\nAirQuality            25\nParks                  0\nBroadband              0\nEdLessColl             0\nSNAPrecipients         1\nMedHomeValue           0\nMedHouseIncome         1\nPoverty                1\nUnemploy               1\nUrbanRural             0\ncounty                 0\nstate                  0\ndtype: int64\nCholMedNonAdhear      80\nCholMedElegible      110\ncruParticipate       742\nCardioPhys          1493\nPhysInactivity        21\nAirQuality            24\npcp                   25\ndtype: int64\n\n\n[False  True]\nbool\n       fips  Age65Plus  AIAN  AIANmen  AIANwomen  ANHPI  ANHPImen  ANHPIwomen  \\\n0     34027       17.1   446      213        233  39922     19200       20722   \n1     34025       17.7   781      508        273  26636     12508       14128   \n2     34017       12.0  2619     1539       1080  88314     44162       44152   \n3     34041       18.1   135       75         60   2178      1107        1071   \n4     34039       14.5  1425      865        560  23289     10781       12508   \n...     ...        ...   ...      ...        ...    ...       ...         ...   \n3135  13157       14.4     7        7          0    948       444         504   \n3136  13153       12.7   904      193        711   4154      1488        2666   \n3137  13139       15.0   326      187        139   3035      1267        1768   \n3138  31019       14.6    69       30         39    652       358         294   \n3139  21101       17.6    75       44         31    160        68          92   \n\n      Black  BlackMen  ...  MedHomeValue  MedHouseIncome  Poverty  Unemploy  \\\n0     13116      6312  ...        462000        114000.0      4.7       5.0   \n1     33615     15605  ...        435000        104000.0      5.9       5.5   \n2     63097     29672  ...        401000         77000.0     13.1       6.8   \n3      3878      1964  ...        266000         80000.0      7.3       5.5   \n4     92839     42103  ...        379000         83000.0      9.2       6.7   \n...     ...       ...  ...           ...             ...      ...       ...   \n3135   3720      1980  ...        210000         71000.0      9.1       2.4   \n3136  35261     16097  ...        157000         69000.0     10.8       3.6   \n3137  11120      5338  ...        213000         68000.0     12.7       2.7   \n3138    369       204  ...        185000         69000.0      9.5       2.0   \n3139   2707      1288  ...        133000         55000.0     13.4       4.5   \n\n             UrbanRural     county  state    pcp  IsRural  CHD_Class  \n0     LargeFringe_Urban     Morris     NJ    1.0    False        Low  \n1     LargeFringe_Urban   Monmouth     NJ    0.8    False        Low  \n2           Large_Urban     Hudson     NJ    1.8    False        Low  \n3     MediumSmall_Urban     Warren     NJ    1.6    False        Low  \n4           Large_Urban      Union     NJ    1.5    False        Low  \n...                 ...        ...    ...    ...      ...        ...  \n3135              Rural    Jackson     GA  140.0     True        Low  \n3136  MediumSmall_Urban    Houston     GA  144.0    False        Low  \n3137  MediumSmall_Urban       Hall     GA  524.0    False        Low  \n3138              Rural    Buffalo     NE  100.0     True        Low  \n3139  MediumSmall_Urban  Henderson     KY   98.0    False     Medium  \n\n[3140 rows x 62 columns]\n       fips  Age65Plus  AIAN  AIANmen  AIANwomen  ANHPI  ANHPImen  ANHPIwomen  \\\n0     34027       17.1   446      213        233  39922     19200       20722   \n1     34025       17.7   781      508        273  26636     12508       14128   \n2     34017       12.0  2619     1539       1080  88314     44162       44152   \n3     34041       18.1   135       75         60   2178      1107        1071   \n4     34039       14.5  1425      865        560  23289     10781       12508   \n...     ...        ...   ...      ...        ...    ...       ...         ...   \n3135  13157       14.4     7        7          0    948       444         504   \n3136  13153       12.7   904      193        711   4154      1488        2666   \n3137  13139       15.0   326      187        139   3035      1267        1768   \n3138  31019       14.6    69       30         39    652       358         294   \n3139  21101       17.6    75       44         31    160        68          92   \n\n      Black  BlackMen  ...  MedHouseIncome  Poverty  Unemploy  \\\n0     13116      6312  ...        114000.0      4.7       5.0   \n1     33615     15605  ...        104000.0      5.9       5.5   \n2     63097     29672  ...         77000.0     13.1       6.8   \n3      3878      1964  ...         80000.0      7.3       5.5   \n4     92839     42103  ...         83000.0      9.2       6.7   \n...     ...       ...  ...             ...      ...       ...   \n3135   3720      1980  ...         71000.0      9.1       2.4   \n3136  35261     16097  ...         69000.0     10.8       3.6   \n3137  11120      5338  ...         68000.0     12.7       2.7   \n3138    369       204  ...         69000.0      9.5       2.0   \n3139   2707      1288  ...         55000.0     13.4       4.5   \n\n             UrbanRural     county  state    pcp  IsRural  CHD_Class  \\\n0     LargeFringe_Urban     Morris     NJ    1.0    False        Low   \n1     LargeFringe_Urban   Monmouth     NJ    0.8    False        Low   \n2           Large_Urban     Hudson     NJ    1.8    False        Low   \n3     MediumSmall_Urban     Warren     NJ    1.6    False        Low   \n4           Large_Urban      Union     NJ    1.5    False        Low   \n...                 ...        ...    ...    ...      ...        ...   \n3135              Rural    Jackson     GA  140.0     True        Low   \n3136  MediumSmall_Urban    Houston     GA  144.0    False        Low   \n3137  MediumSmall_Urban       Hall     GA  524.0    False        Low   \n3138              Rural    Buffalo     NE  100.0     True        Low   \n3139  MediumSmall_Urban  Henderson     KY   98.0    False     Medium   \n\n         region  \n0     Northeast  \n1     Northeast  \n2     Northeast  \n3     Northeast  \n4     Northeast  \n...         ...  \n3135      South  \n3136      South  \n3137      South  \n3138    Midwest  \n3139      South  \n\n[3140 rows x 63 columns]\n\n\nFollowing this, we performed an exploratory data analysis to understand the distribution of the data and the relationships between different variables.\n\n\nShow the code\nchd=cdc\n\n#Scatter plot for a variety of variables against variable of interest\n# Create a 3x4 grid of subplots (12 subplots in total)\nnum_rows = 3\nnum_cols = 4\n\n# Adjust the figure size according to your preference\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 8))\n\n# Flatten the axes array for easier iteration\naxes = axes.ravel()\n\ncol_list = ('Stroke', 'Diabetes', 'Smoker', 'PhysInactivity',\n            'HealthIns', 'HospCR', 'Pharmacies', 'AirQuality',\n            'Poverty', 'MedHouseIncome', 'UrbanRural', 'Parks'\n            )\n\n# Plot each independent variable against the dependent variable in each subplot\nfor i, ax in enumerate(axes):\n    ax.scatter(chd[col_list[i]], chd.CHD, s=1, alpha=0.1)\n    ax.set_title(f\"{col_list[i]}\")\n    ax.set_ylabel(\"CHD\")\n\n# Adjust spacing between subplots for better visualization\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\nimport matplotlib.pyplot as plt2\n\n# Create a heatmap with automatically binned continuous variables and average Z value in each bin\ndef create_heatmap(x_data, y_data, z_data, x_label, y_label, z_label):\n    # Combine the data into a DataFrame\n    data = pd.DataFrame({'X': x_data, 'Y': y_data, 'Z': z_data})\n\n    # Determine the number of bins (you can adjust this as needed)\n    num_bins = 20\n\n    # Use pandas cut function to create bins for X and Y variables\n    data['X_bin'] = pd.cut(data['X'], bins=num_bins, labels=False)\n    data['Y_bin'] = pd.cut(data['Y'], bins=num_bins, labels=False)\n\n    # Group the data by the bins and calculate the average Z value in each bin\n    heatmap_data = data.groupby(['X_bin', 'Y_bin'])['Z'].mean().unstack()\n\n    # Create the heatmap using a separate figure\n    plt2.figure(figsize=(8, 6))\n    sns.heatmap(heatmap_data, cmap='viridis', annot=False, fmt=\".2f\", cbar_kws={'label': z_label})\n    plt2.xlabel(x_label)\n    plt2.ylabel(y_label)\n    plt2.title(f'Heatmap of {z_label} by {x_label} and {y_label}')\n    plt2.show()\n\ncreate_heatmap(chd.PhysInactivity, chd.Poverty, chd.CHD, 'PhysInactivity', 'Poverty', 'Avg. CHD')\n\n\n\n\n\n\n\n\nNext, we split the data into a training set and a test set. We trained SVM and Random Forest models using the training data, tuning their hyperparameters with GridSearchCV and RandomizedSearchCV respectively. In addition to these models, we also employed a k-means clustering algorithm to explore potential clusters in the dataset.\nRandom Forest:\n\n\nShow the code\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import randint\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nchd = pd.read_csv(\"data/CDC_python_clean.csv\")\n\n### Drop unneeded columns\ncolumns_to_drop = ['fips', 'CHD', 'county','UrbanRural']\nchd = chd.drop(columns=columns_to_drop)\n\n### Establish X and y\nX = chd.drop(columns=['CHD_Class'])\ny = chd['CHD_Class']\n\n### Additional preprocessing\n# Get the character columns that need to be dummy encoded\nchar_cols = X.select_dtypes(include=['object']).columns\n\n# Create a one-hot encoder and apply it to the character columns\nencoder = OneHotEncoder()\nX = pd.get_dummies(X, columns=char_cols, drop_first=True)\n\n# Scale and Center\nnumeric_columns = X.select_dtypes(include=['float64', 'int64']).columns\nscaler = StandardScaler()\nX[numeric_columns] = scaler.fit_transform(X[numeric_columns])\n\n### Split data\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=42)\n\n### Establish Parameters\nrf_classifier = RandomForestClassifier()\n\nparam_dist = {\n    'n_estimators': randint(1, 1000),\n    'max_depth': randint(1, 50),\n    'min_samples_split': randint(2, 10),\n    'min_samples_leaf': randint(1, 10),\n    'bootstrap': [True, False],\n    'criterion': ['gini', 'entropy']\n}\n\n### Additional Parameters\nrandom_search = RandomizedSearchCV(\n    estimator=rf_classifier,\n    param_distributions=param_dist,\n    n_iter=50,  # Adjust the number of iterations as needed\n    cv=5,        # Number of cross-validation folds\n    n_jobs=-1,   # Use all available CPU cores for parallel processing\n    random_state=42,\n    verbose=True\n)\n\n#Model\nrandom_search.fit(Xtrain, ytrain)\n\n### Store best model\nbest_rf_classifier = random_search.best_estimator_\n\naccuracy = best_rf_classifier.score(Xtest, ytest)\nprint(\"Accuracy of the Random Forest Classifier: {:.2f}%\".format(accuracy * 100))\n\n#Feature Importance\nfeature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': best_rf_classifier.feature_importances_})\n\ntop_20_features = feature_importance_df.nlargest(50, 'Importance')\n\nimport matplotlib.pyplot as plt\n\n# Create a horizontal bar plot with rotated axes\nplt.figure(figsize=(8, 12))\nplt.barh(top_20_features['Feature'], top_20_features['Importance'], color='darkgreen')\nplt.gca().invert_yaxis()  # Invert the y-axis to show features at the top\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.title('Top 50 Variable Importance in Random Forest Classifier')\nplt.tight_layout()\nplt.show()\n\n\nfrom sklearn.metrics import confusion_matrix\n\nypred = best_rf_classifier.predict(Xtest)\n\nconf_matrix = confusion_matrix(ytest, ypred)\n\nclass_names = ['low', 'medium', 'high']\nconf_matrix_df = pd.DataFrame(conf_matrix, index=class_names, columns=class_names)\n\n## Confussion matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Set the labels for the heatmap\nlabels = ['High','Medium','Low']\n\n# Create the heatmap\nplt.figure(figsize=(6, 4))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix Heatmap')\nplt.show()\n\n### Split Urban Rural and Redo Analysis\ndef get_variable_importance(data):\n    # Separate the target variable 'CHD_Class' from the features\n    X = data.drop(columns=['CHD_Class'])\n    y = data['CHD_Class']\n    \n    # Get the character columns that need to be dummy encoded\n    char_cols = X.select_dtypes(include=['object']).columns\n\n    # Create a one-hot encoder and apply it to the character columns\n    encoder = OneHotEncoder()\n    X = pd.get_dummies(X, columns=char_cols, drop_first=True)\n\n    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=42)\n    \n    rf_classifier = RandomForestClassifier()\n\n    param_dist = {\n        'n_estimators': randint(1, 1000),\n        'max_depth': randint(1, 50),\n        'min_samples_split': randint(2, 10),\n        'min_samples_leaf': randint(1, 10),\n        'bootstrap': [True, False],\n        'criterion': ['gini', 'entropy']\n    }\n    \n    \n    random_search = RandomizedSearchCV(\n        estimator=rf_classifier,\n        param_distributions=param_dist,\n        n_iter=50,  # Adjust the number of iterations as needed\n        cv=5,        # Number of cross-validation folds\n        n_jobs=-1,   # Use all available CPU cores for parallel processing\n        random_state=42,\n        verbose=True\n    )\n\n    random_search.fit(Xtrain, ytrain)\n    \n    best_rf_classifier = random_search.best_estimator_\n    \n    accuracy = best_rf_classifier.score(Xtest, ytest)\n    print(\"Accuracy of the Random Forest Classifier: {:.2f}%\".format(accuracy * 100))\n    \n    # Create a DataFrame to store the feature names and their corresponding importance scores\n    feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': best_rf_classifier.feature_importances_})\n\n    # Sort the DataFrame in descending order based on importance\n    feature_importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n\n    return feature_importance_df\n  \n# Create separate datasets for IsRural=True and IsRural=False\ndata_rural = chd[chd['IsRural'] == True]\ndata_non_rural = chd[chd['IsRural'] == False]\n\n# Get variable importance for IsRural=True\nresult_rural = get_variable_importance(data_rural)\n\n# Get variable importance for IsRural=False\nresult_non_rural = get_variable_importance(data_non_rural)\n\n# Add a new column to each DataFrame to indicate the dataset (IsRural=True or IsRural=False)\nresult_rural['Dataset'] = 'IsRural=True'\nresult_non_rural['Dataset'] = 'IsRural=False'\n\n# Concatenate the two DataFrames into a single DataFrame\ncombined_result = pd.concat([result_rural, result_non_rural,], ignore_index=True)\n\ncombined_result = combined_result[combined_result['Importance'] > 0.01]\n\n# Create a bar plot with rotated axes\nplt.figure(figsize=(10, 10))\nsns.barplot(x='Importance', y='Feature', hue='Dataset', data=combined_result)\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.title('Variable Importance Comparison between IsRural=True and IsRural=False')\nplt.legend(title='Dataset', loc='lower right')\nplt.tight_layout()\nplt.show()\n\n\nFitting 5 folds for each of 50 candidates, totalling 250 fits\n\n\nAccuracy of the Random Forest Classifier: 88.54%\n\n\n\n\n\n\n\n\nFitting 5 folds for each of 50 candidates, totalling 250 fits\n\n\nAccuracy of the Random Forest Classifier: 87.25%\nFitting 5 folds for each of 50 candidates, totalling 250 fits\n\n\nAccuracy of the Random Forest Classifier: 88.70%\n\n\n\n\n\nK-Means:\n\n\nShow the code\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\ndf = pd.read_csv('data/CDC_python_clean.csv')\ndf = df.loc[df.CHD > 0]\n\n### Selecting columns\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Select numerical columns\nnumerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n\n# Remove the 'fips' column as it's a unique identifier, not a meaningful numerical feature\nnumerical_cols = numerical_cols.drop('fips')\n\n# Subset the dataframe on these columns\ndf_numerical = df[numerical_cols]\n\n# Normalize the data\nscaler = StandardScaler()\ndf_normalized = pd.DataFrame(scaler.fit_transform(df_numerical), columns=df_numerical.columns)\n\ndf_normalized.head()\n\n### Determining how many clusters to use\n\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Set a range of clusters to try out\nclusters_range = range(2, 15)\n\n# List to hold the inertia for each number of clusters\ninertias = []\n\n# Perform K-means for each number of clusters and store the inertia\nfor num_clusters in clusters_range:\n    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n    kmeans.fit(df_normalized)\n    inertias.append(kmeans.inertia_)\n\n# Plot the elbow plot\nplt.figure(figsize=(8, 6))\nplt.plot(clusters_range, inertias, 'bo-')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Inertia')\nplt.title('Elbow Method For Optimal Number of Clusters')\nplt.grid(True)\nplt.show()\n\n# The Elbow Method plot shows the inertia (sum of squared distances to the nearest cluster center) as a function of the number of clusters. From the plot, it's not entirely clear where the \"elbow\" is, as there isn't a sharp bend. This often happens with real-world data, which may not have well-separated clusters. However, we can see that the inertia starts to decrease at a slower rate from around 4 clusters onwards. Therefore, we'll choose 4 as the number of clusters for our K-means clustering.\n\n# Perform K-means clustering with 4 clusters\nkmeans = KMeans(n_clusters=4, random_state=42)\nkmeans.fit(df_normalized)\n\n# Get the cluster assignments for each data point\ncluster_assignments = kmeans.labels_\n\n# Add the cluster assignments back to the original DataFrame\ndf['Cluster'] = cluster_assignments\n\ndf.head()\n\n# Calculate the mean values of our features within each cluster\ncluster_characteristics = df.groupby('Cluster').mean()\ncluster_characteristics.transpose()\n\nfrom sklearn.metrics import silhouette_score\n\n# Calculate the silhouette score\nsil_score = silhouette_score(df_normalized, cluster_assignments)\n\nsil_score\n\n# The silhouette score for our clustering is approximately 0.167. This score is relatively low, indicating that the clusters are not very clearly separated and that the samples within each cluster are not extremely dense. This is not surprising considering that we have used a high-dimensional dataset with many features, which can make it challenging to form distinct clusters.\n\n\n\n\n\n0.16659025768870228\n\n\nSVC:\n\n\nShow the code\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n# Load data\nchd = pd.read_csv(\"data/CDC_python_clean.csv\")\n\n# Drop unnecessary columns\ncolumns_to_drop = ['fips', 'CHD', 'county', 'UrbanRural']\nchd = chd.drop(columns=columns_to_drop)\n\n# Define features and target\nX = chd.drop(columns=['CHD_Class'])\ny = chd['CHD_Class']\n\n# One-hot encode categorical variables\nchar_cols = X.select_dtypes(include=['object']).columns\nX = pd.get_dummies(X, columns=char_cols, drop_first=True)\n\n# Scale numerical variables\nnumeric_columns = X.select_dtypes(include=['float64', 'int64']).columns\nscaler = StandardScaler()\nX[numeric_columns] = scaler.fit_transform(X[numeric_columns])\n\nX.head(), y.head()\n\n# Splitting the data\n\nfrom sklearn.model_selection import train_test_split\n\n# Split data into train and test sets\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=42)\n\nXtrain.shape, Xtest.shape\n\n#Model Building and Hyperparameter Tuning\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\n# Define the SVC classifier\nsvc = SVC(kernel='linear')\n\n# Define the parameter grid for the GridSearchCV\nparam_grid = {\n    'C': [0.1, 1],\n}\n\n# Define the GridSearchCV for hyperparameter tuning\ngrid_search = GridSearchCV(\n    estimator=svc,\n    param_grid=param_grid,\n    cv=5,        # Number of cross-validation folds\n    n_jobs=-1,   # Use all available CPU cores for parallel processing\n    verbose=True\n)\n\n# Fit the GridSearchCV to the training data\ngrid_search.fit(Xtrain, ytrain)\n\n# Model Evaluation\n\nfrom sklearn.metrics import accuracy_score\n\n# Get the best SVC classifier\nbest_svc = grid_search.best_estimator_\n\n# Predict the test set results\nsvc_pred = best_svc.predict(Xtest)\n\n# Compute the accuracy of the SVC classifier\nsvc_accuracy = accuracy_score(ytest, svc_pred)\n\nsvc_accuracy\n\n# Feature Importance Plot\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Get feature importances (use absolute values of coefficients as proxy)\nimportances = abs(best_svc.coef_[0])\n\n# Get top 10 features\nindices = np.argsort(importances)[-10:]\n\nplt.figure(figsize=(12, 8))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [X.columns[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()\n\n# Confusion Matrix and Plot\n# Import the required libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Use the sklearn function confusion_matrix to compute the confusion matrix\nsvc_cm = confusion_matrix(ytest, svc_pred)\n\n# Using seaborn to create a heatmap. The heatmap will visually represent the confusion matrix\n# Each cell in the heatmap corresponds to a cell in the confusion matrix.\n# The color of the cell is proportional to the number of instances.\nsns.heatmap(svc_cm, annot=True, fmt='d')\n\n# Label the x-axis as 'Predicted'\nplt.xlabel('Predicted')\n\n# Label the y-axis as 'True'\nplt.ylabel('True')\n\n\nFitting 5 folds for each of 2 candidates, totalling 10 fits\n\n\n\n\n\nText(55.333333333333336, 0.5, 'True')\n\n\n\n\n\nThe performance of the SVM and Random Forest models was evaluated based on accuracy on the test data, while the k-means clustering results were assessed visually and qualitatively.\n\n\nShow the code\nsvc_accuracy = accuracy_score(ytest, svc_pred)\nprint(\"Accuracy of the Support Vector Classifier: {:.2f}%\".format(svc_accuracy * 100))\naccuracy = best_rf_classifier.score(Xtest, ytest)\nprint(\"Accuracy of the Random Forest Classifier: {:.2f}%\".format(accuracy * 100))\nsil_score = silhouette_score(df_normalized, cluster_assignments)\nprint(\"Accuracy of the K-Means Cluster: {:.2f}%\".format(sil_score * 100))\n\n\nAccuracy of the Support Vector Classifier: 92.10%\nAccuracy of the Random Forest Classifier: 88.54%\nAccuracy of the K-Means Cluster: 16.66%"
  },
  {
    "objectID": "posts/Python Group Project/index.html#results",
    "href": "posts/Python Group Project/index.html#results",
    "title": "Healthcare Access and its Effects on Coronary Heart Disease Prevalence",
    "section": "Results:",
    "text": "Results:\nThrough our exploratory data analysis, we found interesting patterns and distributions in the data, such as, you can see a positive correlation between coronary heart disease (CHD) and people who smoke, and with people who have had a stroke. Also see that as poverty rates increase there is an association that prevalence of CHD also goes up. When you look at median household income, as income rises prevalence of coronary heart disease goes down.\nOur machine learning models yielded promising results. The SVM model achieved an accuracy of approximately 92.1%, while the Random Forest model achieved an accuracy of approximately 89.4%. Therefore, the SVM model performed slightly better on this dataset. The k-means clustering shows a silhouette score of approximately 0.167. This score is relatively low, indicating that the clusters are not very clearly separated and that the samples within each cluster are not extremely dense. This is not surprising considering that we have used a high-dimensional dataset with many features, which can make it challenging to form distinct clusters.."
  },
  {
    "objectID": "posts/Python Group Project/index.html#discussions-and-implications",
    "href": "posts/Python Group Project/index.html#discussions-and-implications",
    "title": "Healthcare Access and its Effects on Coronary Heart Disease Prevalence",
    "section": "Discussions and Implications:",
    "text": "Discussions and Implications:\nThe difference in performance between the SVM and Random Forest models could be attributed to the specific properties of these algorithms. SVMs tend to perform well on high-dimensional data, while Random Forests are often better suited for datasets with a mix of categorical and numerical features. The k-means clustering results provide additional insights into the structure of the dataset, potentially informing feature engineering and selection processes.\nThese findings highlight the potential of machine learning in aiding the prediction of Coronary Heart Disease, which could have significant implications for early intervention and treatment planning."
  },
  {
    "objectID": "posts/Python Group Project/index.html#conclusion",
    "href": "posts/Python Group Project/index.html#conclusion",
    "title": "Healthcare Access and its Effects on Coronary Heart Disease Prevalence",
    "section": "Conclusion:",
    "text": "Conclusion:\nIn conclusion, our project demonstrates the feasibility and potential of using machine learning for predicting Coronary Heart Disease. The findings suggest that machine learning, and specifically SVM, can provide a valuable tool in the field of health informatics. Future work could explore other algorithms, feature engineering techniques, and larger or more diverse datasets to further improve prediction performance.These findings suggest that machine learning can be a valuable tool in health informatics, providing insights that can aid in early disease prediction and intervention."
  },
  {
    "objectID": "posts/Python Group Project/index.html#references",
    "href": "posts/Python Group Project/index.html#references",
    "title": "Healthcare Access and its Effects on Coronary Heart Disease Prevalence",
    "section": "References:",
    "text": "References:\n\nScikit-learn: Machine Learning in Python (https://scikit-learn.org/stable/index.html)\nCDC Dataset (Link to CDC Dataset)"
  },
  {
    "objectID": "posts/Python Group Project/index.html#appendix-list-of-contributors",
    "href": "posts/Python Group Project/index.html#appendix-list-of-contributors",
    "title": "Healthcare Access and its Effects on Coronary Heart Disease Prevalence",
    "section": "Appendix: List of Contributors",
    "text": "Appendix: List of Contributors\n\nHans LehnDorff - LinkedIn\nIsaac Johnson - LinkedIn\nJesse Debolt - LinkedIn"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Isaac Johnson’s Portfolio",
    "section": "",
    "text": "I am excited to share with you some of my progress so far. I am currently enrolled in a Masters of Data Science program at Willamette University, and looking forward to graduation this August ’23! Please take a look around and would love to hear from you."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I making a career pivot, and working on something I have wanted to do for a long time. Currently enrolled at Willamette Univeristy in their Master’s of Data Science program, and am expecting to graduate this August ’23. Previously I worked in healthcare as a medical and pharmaceutical sales representative. Through that I got to learn many aspects of working in healthcare very thoroughly, including health insurance, formularies, prior authorizations, managed care plans, communicating with doctors, staff, and many other stakeholders.\nmore to come….."
  },
  {
    "objectID": "posts/Python Group Project/CHD Final Report.html#introduction",
    "href": "posts/Python Group Project/CHD Final Report.html#introduction",
    "title": "Healthcare Access and its Effects on Coronary Heart Disease Prevalence",
    "section": "Introduction:",
    "text": "Introduction:\nThis project aims to leverage machine learning techniques to predict the occurrence of Coronary Heart Disease (CHD), a leading cause of death globally. We believe that early prediction of CHD can lead to preventative measures, better health outcomes, and potentially save lives. Our approach involves using health and demographic data to train predictive models, specifically utilizing Support Vector Machines (SVM), Random Forests, and k-means clustering algorithms. The models are trained and evaluated using a dataset from the CDC, which includes information such as age, gender, cholesterol levels, and other health indicators."
  },
  {
    "objectID": "posts/Python Group Project/CHD Final Report.html#problem-statement",
    "href": "posts/Python Group Project/CHD Final Report.html#problem-statement",
    "title": "Healthcare Access and its Effects on Coronary Heart Disease Prevalence",
    "section": "Problem Statement:",
    "text": "Problem Statement:\nCoronary Heart Disease is a leading cause of death globally. Early prediction of the disease can lead to early intervention and potentially save lives. The challenge is to build an accurate and reliable predictive model using available health and demographic data."
  },
  {
    "objectID": "posts/Python Group Project/CHD Final Report.html#research-questions",
    "href": "posts/Python Group Project/CHD Final Report.html#research-questions",
    "title": "Healthcare Access and its Effects on Coronary Heart Disease Prevalence",
    "section": "Research Questions:",
    "text": "Research Questions:\n\nCan we predict the presence of Coronary Heart Disease using health and demographic data?\nHow we’ll do SVM, Random Forest, and k-means clustering algorithms perform in predicting CHD?\nWhich features are most predictive of CHD according to our models?"
  },
  {
    "objectID": "posts/Python Group Project/CHD Final Report.html#methodology",
    "href": "posts/Python Group Project/CHD Final Report.html#methodology",
    "title": "Healthcare Access and its Effects on Coronary Heart Disease Prevalence",
    "section": "Methodology:",
    "text": "Methodology:\nOur methodology involves several key steps. First, we preprocessed the dataset obtained from the CDC, which involved handling missing values, one-hot encoding categorical variables, and scaling numerical variables.\n\n\nShow the code\n#Import needed libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n#Importing data\ncdc = pd.read_csv('data/CDC_for_python.csv')\ncdc = cdc.loc[cdc.CHD > 0]\n\n#View data structure\ncdc.shape\n\n#### Data cleaning ####\ncdc = cdc[~cdc['display_name'].str.contains(r'\\(AS\\)|\\(GU\\)|\\(MP\\)|\\(PR\\)|\\(County Equivalent\\)')]\n\n# Separate the 'display_name' column\ncdc[['county', 'state']] = cdc['display_name'].str.extract(r'\\\"(.+), \\((.+)\\)\\\"', expand=True)\n\n# Remove the original column\ncdc = cdc.drop(['display_name'], axis=1)\n\n#change rural/urban\n# 1 = Large central metro -> Large_Urban\n# 2 = Large fringe metro -> LargeFringe_Urban\n# 3 = Medium/small metro -> MediumSmall_Urban\n# 4 = Nonmetro -> Rural\n\n# Replace values in 'UrbanRural' column\ncdc['UrbanRural'] = cdc['UrbanRural'].replace({1: 'Large_Urban', 2: 'LargeFringe_Urban', 3: 'MediumSmall_Urban', 4: 'Rural'})\n\n# Replace -1 with NaN\ncdc = cdc.replace(-1, np.nan)\n\n# Display column names\nprint(cdc.columns)\n\n# Show unique values in 'UrbanRural'\nprint(cdc['UrbanRural'].unique())\n\n# Create a subset where 'UrbanRural' is NaN, an empty string, or 'NA'\nrural_query = cdc[cdc['UrbanRural'].isin([pd.np.nan, '', 'NA'])]\n\n# Display the first 20 rows of this subset\nrural_query.head(20)\n\n# Find and insert where county is 'Kusilvak' based on Wikipedia data.\ncdc.loc[cdc['county'].str.contains('Kusilvak', na=False), 'UrbanRural'] = \"Rural\"\n\n# Display unique values in the 'UrbanRural' column\nprint(cdc['UrbanRural'].unique())\n\n# Double check for any NAs in Rural/Urban\nrural_query = cdc[cdc['UrbanRural'].isin([None, \"\", \"NA\"])].head(20)\nrural_query\n\n# Convert fips to string\ncdc['fips'] = cdc['fips'].astype(str)\n\n# Inserting Parks missing value\ncdc.loc[cdc['county'].str.contains('Kusilvak', na=False), 'Parks'] = 66\n\n# Total number of missing values in dataset\ntotal_na = cdc.isnull().sum().sum()\nprint(total_na)\n\n# Total number of missing values in each column\ncolumn_na = cdc.isnull().sum()\nprint(column_na)\n\n# Total number of missing values in each row\ncdc['count_na'] = cdc.isnull().sum(axis=1)\n\n# Sort by 'count_na'\ncdc = cdc.sort_values(by='count_na', ascending=False)\n\n# Remove those that have more than 8 NAs in that row\ncdc = cdc[cdc['count_na'] <= 8]\n\n# Remove 'count_na' column\ncdc = cdc.drop('count_na', axis=1)\n\n## Inserting values for missing data for NJ\n# bpmUse\ncdc.loc[cdc['state'].str.contains('NJ', na=False), 'bpmUse'] = 71.71\n\n# CholScreen\ncdc.loc[cdc['state'].str.contains('NJ', na=False), 'CholScreen'] = 79.43\n\n# HighBP\ncdc.loc[cdc['state'].str.contains('NJ', na=False), 'HighBP'] = 33.7\n\n# Diabetes\ncdc.loc[cdc['state'].str.contains('NJ', na=False), 'Diabetes'] = 17.4\n\n# HighChol\ncdc.loc[cdc['state'].str.contains('NJ', na=False), 'HighChol'] = 32.41\n\n# Obesity\ncdc.loc[cdc['state'].str.contains('NJ', na=False), 'Obesity'] = 33.59\n\n## Inserting values for missing values in Median Home Value\ncdc.loc[cdc['fips'].str.contains('48261', na=False), 'MedHomeValue'] = 42550\ncdc.loc[cdc['fips'].str.contains('48301', na=False), 'MedHomeValue'] = 38143\ncdc.loc[cdc['fips'].str.contains('46017', na=False), 'MedHomeValue'] = 101393\ncdc.loc[cdc['fips'].str.contains('46095', na=False), 'MedHomeValue'] = 60537\n\n## Inserting missing values for PCP and Cardio Phys\n# Read in dataset with values for merging\npcp_cardio_count = pd.read_csv(\"data/pcp_cardio_count.csv\")\n\n# Convert the 'COUNTY' column to string\npcp_cardio_count['COUNTY'] = pcp_cardio_count['COUNTY'].astype(str)\n\n# Joining count data frame to the cdc data frame\ncdc = cdc.merge(pcp_cardio_count, left_on='fips', right_on='COUNTY', how='left')\n\n# Use fillna to replace NAs in PCP and CardioPhys\ncdc['pcp'] = cdc['pcp'].fillna(cdc['PrimaryCarePhys'])\ncdc['CardioPhys'] = cdc['CardioPhys'].fillna(cdc['cardio'])\n\n# Remove the temporary columns\ncdc = cdc.drop(['PrimaryCarePhys', 'cardio', 'COUNTY'], axis=1)\n\n# Check DataFrame for missing or NA values, but only display columns with missing values\nmissing_values = cdc.isnull().sum()\nmissing_values = missing_values[missing_values != 0]\nprint(missing_values)\n\n#### Data Engineering ####\n#Impute for missing values\nfrom sklearn.impute import SimpleImputer\n\n# Specify the columns you want to impute\ncolumns_to_impute = ['CholMedNonAdhear', 'CholMedElegible', 'cruParticipate', 'CardioPhys', 'PhysInactivity', 'AirQuality', 'pcp']\n\n# Subset the DataFrame to only these columns\nsubset_cdc = cdc[columns_to_impute]\n\n# Create an imputer object\nimputer = SimpleImputer(strategy='mean')\n\n# Fit the imputer to the data and transform the data\nimputed_data = imputer.fit_transform(subset_cdc)\n\n# Convert the result back to a DataFrame\nimputed_data = pd.DataFrame(imputed_data, columns=subset_cdc.columns)\n\n# Replace the original columns in the DataFrame with the imputed data\ncdc[columns_to_impute] = imputed_data\n\ncdc['IsRural'] = cdc['UrbanRural'] == 'Rural'\n\n# Checking status of new column\nprint(cdc.IsRural.unique())\nprint(cdc['IsRural'].dtype)\n\n#Create classes for CHD prevalence\n# Define the boundaries for the quantiles\nquantiles = cdc['CHD'].quantile([0.33, 0.66]).values\n\n# Function to classify 'CHD' values\ndef classify_chd(value):\n    if value <= quantiles[0]:\n        return 'Low'\n    elif value <= quantiles[1]:\n        return 'Medium'\n    else:\n        return 'High'\n\n# Add 'CHD_Class' column\ncdc['CHD_Class'] = cdc['CHD'].apply(classify_chd)\n\n# Print the dataframe\nprint(cdc)\n\n# Define the regions\nregions = {\n    'Northeast': ['CT', 'ME', 'MA', 'NH', 'RI', 'VT', 'NJ', 'NY', 'PA'],\n    'Midwest': ['IL', 'IN', 'MI', 'OH', 'WI', 'IA', 'KS', 'MN', 'MO', 'NE', 'ND', 'SD'],\n    'South': ['DE', 'FL', 'GA', 'MD', 'NC', 'SC', 'VA', 'DC', 'WV', 'AL', 'KY', 'MS', 'TN', 'AR', 'LA', 'OK', 'TX'],\n    'West': ['AZ', 'CO', 'ID', 'MT', 'NV', 'NM', 'UT', 'WY', 'AK', 'CA', 'HI', 'OR', 'WA']\n}\n\n# Function to assign region based on state\ndef assign_region(state):\n    for region, states in regions.items():\n        if state in states:\n            return region\n    return 'Other'\n\n# Add 'region' column\ncdc['region'] = cdc['state'].apply(assign_region)\n\n# Print the updated DataFrame\nprint(cdc)\n\n\nIndex(['fips', 'Age65Plus', 'AIAN', 'AIANmen', 'AIANwomen', 'ANHPI',\n       'ANHPImen', 'ANHPIwomen', 'Black', 'BlackMen', 'BlackWomen', 'Hispanic',\n       'HispanicMen', 'HispanicWomen', 'OtherRace', 'OtherRaceMen',\n       'OtherRaceWomen', 'PopAllGenders', 'PopMen', 'PopWomen', 'pop',\n       'TwoPlus', 'TwoPlusMen', 'TwoPlusWomen', 'White', 'WhiteMen',\n       'WhiteWomen', 'bpmUse', 'CholScreen', 'CholMedNonAdhear',\n       'CholMedElegible', 'cruParticipate', 'Hospitals', 'HospCIC', 'HospCR',\n       'HospED', 'Pharmacies', 'HealthIns', 'CardioPhys', 'PrimaryCarePhys',\n       'CHD', 'HighBP', 'Stroke', 'Diabetes', 'HighChol', 'Obesity',\n       'PhysInactivity', 'Smoker', 'AirQuality', 'Parks', 'Broadband',\n       'EdLessColl', 'SNAPrecipients', 'MedHomeValue', 'MedHouseIncome',\n       'Poverty', 'Unemploy', 'UrbanRural', 'county', 'state'],\n      dtype='object')\n['Rural' 'MediumSmall_Urban' nan 'Large_Urban' 'LargeFringe_Urban']\n['Rural' 'MediumSmall_Urban' 'Large_Urban' 'LargeFringe_Urban']\n3297\nfips                   0\nAge65Plus              0\nAIAN                   0\nAIANmen                0\nAIANwomen              0\nANHPI                  0\nANHPImen               0\nANHPIwomen             0\nBlack                  0\nBlackMen               0\nBlackWomen             0\nHispanic               0\nHispanicMen            0\nHispanicWomen          0\nOtherRace              0\nOtherRaceMen           0\nOtherRaceWomen         0\nPopAllGenders          0\nPopMen                 0\nPopWomen               0\npop                    0\nTwoPlus                0\nTwoPlusMen             0\nTwoPlusWomen           0\nWhite                  0\nWhiteMen               0\nWhiteWomen             0\nbpmUse                21\nCholScreen            21\nCholMedNonAdhear      81\nCholMedElegible      111\ncruParticipate       743\nHospitals              0\nHospCIC                0\nHospCR                 0\nHospED                 0\nPharmacies             0\nHealthIns              1\nCardioPhys          1965\nPrimaryCarePhys      220\nCHD                    0\nHighBP                21\nStroke                 0\nDiabetes              21\nHighChol              21\nObesity               21\nPhysInactivity        21\nSmoker                 0\nAirQuality            25\nParks                  0\nBroadband              0\nEdLessColl             0\nSNAPrecipients         1\nMedHomeValue           0\nMedHouseIncome         1\nPoverty                1\nUnemploy               1\nUrbanRural             0\ncounty                 0\nstate                  0\ndtype: int64\nCholMedNonAdhear      80\nCholMedElegible      110\ncruParticipate       742\nCardioPhys          1493\nPhysInactivity        21\nAirQuality            24\npcp                   25\ndtype: int64\n[False  True]\nbool\n       fips  Age65Plus  AIAN  AIANmen  AIANwomen  ANHPI  ANHPImen  ANHPIwomen  \\\n0     34027       17.1   446      213        233  39922     19200       20722   \n1     34025       17.7   781      508        273  26636     12508       14128   \n2     34017       12.0  2619     1539       1080  88314     44162       44152   \n3     34041       18.1   135       75         60   2178      1107        1071   \n4     34039       14.5  1425      865        560  23289     10781       12508   \n...     ...        ...   ...      ...        ...    ...       ...         ...   \n3135  13157       14.4     7        7          0    948       444         504   \n3136  13153       12.7   904      193        711   4154      1488        2666   \n3137  13139       15.0   326      187        139   3035      1267        1768   \n3138  31019       14.6    69       30         39    652       358         294   \n3139  21101       17.6    75       44         31    160        68          92   \n\n      Black  BlackMen  ...  MedHomeValue  MedHouseIncome  Poverty  Unemploy  \\\n0     13116      6312  ...        462000        114000.0      4.7       5.0   \n1     33615     15605  ...        435000        104000.0      5.9       5.5   \n2     63097     29672  ...        401000         77000.0     13.1       6.8   \n3      3878      1964  ...        266000         80000.0      7.3       5.5   \n4     92839     42103  ...        379000         83000.0      9.2       6.7   \n...     ...       ...  ...           ...             ...      ...       ...   \n3135   3720      1980  ...        210000         71000.0      9.1       2.4   \n3136  35261     16097  ...        157000         69000.0     10.8       3.6   \n3137  11120      5338  ...        213000         68000.0     12.7       2.7   \n3138    369       204  ...        185000         69000.0      9.5       2.0   \n3139   2707      1288  ...        133000         55000.0     13.4       4.5   \n\n             UrbanRural     county  state    pcp  IsRural  CHD_Class  \n0     LargeFringe_Urban     Morris     NJ    1.0    False        Low  \n1     LargeFringe_Urban   Monmouth     NJ    0.8    False        Low  \n2           Large_Urban     Hudson     NJ    1.8    False        Low  \n3     MediumSmall_Urban     Warren     NJ    1.6    False        Low  \n4           Large_Urban      Union     NJ    1.5    False        Low  \n...                 ...        ...    ...    ...      ...        ...  \n3135              Rural    Jackson     GA  140.0     True        Low  \n3136  MediumSmall_Urban    Houston     GA  144.0    False        Low  \n3137  MediumSmall_Urban       Hall     GA  524.0    False        Low  \n3138              Rural    Buffalo     NE  100.0     True        Low  \n3139  MediumSmall_Urban  Henderson     KY   98.0    False     Medium  \n\n[3140 rows x 62 columns]\n       fips  Age65Plus  AIAN  AIANmen  AIANwomen  ANHPI  ANHPImen  ANHPIwomen  \\\n0     34027       17.1   446      213        233  39922     19200       20722   \n1     34025       17.7   781      508        273  26636     12508       14128   \n2     34017       12.0  2619     1539       1080  88314     44162       44152   \n3     34041       18.1   135       75         60   2178      1107        1071   \n4     34039       14.5  1425      865        560  23289     10781       12508   \n...     ...        ...   ...      ...        ...    ...       ...         ...   \n3135  13157       14.4     7        7          0    948       444         504   \n3136  13153       12.7   904      193        711   4154      1488        2666   \n3137  13139       15.0   326      187        139   3035      1267        1768   \n3138  31019       14.6    69       30         39    652       358         294   \n3139  21101       17.6    75       44         31    160        68          92   \n\n      Black  BlackMen  ...  MedHouseIncome  Poverty  Unemploy  \\\n0     13116      6312  ...        114000.0      4.7       5.0   \n1     33615     15605  ...        104000.0      5.9       5.5   \n2     63097     29672  ...         77000.0     13.1       6.8   \n3      3878      1964  ...         80000.0      7.3       5.5   \n4     92839     42103  ...         83000.0      9.2       6.7   \n...     ...       ...  ...             ...      ...       ...   \n3135   3720      1980  ...         71000.0      9.1       2.4   \n3136  35261     16097  ...         69000.0     10.8       3.6   \n3137  11120      5338  ...         68000.0     12.7       2.7   \n3138    369       204  ...         69000.0      9.5       2.0   \n3139   2707      1288  ...         55000.0     13.4       4.5   \n\n             UrbanRural     county  state    pcp  IsRural  CHD_Class  \\\n0     LargeFringe_Urban     Morris     NJ    1.0    False        Low   \n1     LargeFringe_Urban   Monmouth     NJ    0.8    False        Low   \n2           Large_Urban     Hudson     NJ    1.8    False        Low   \n3     MediumSmall_Urban     Warren     NJ    1.6    False        Low   \n4           Large_Urban      Union     NJ    1.5    False        Low   \n...                 ...        ...    ...    ...      ...        ...   \n3135              Rural    Jackson     GA  140.0     True        Low   \n3136  MediumSmall_Urban    Houston     GA  144.0    False        Low   \n3137  MediumSmall_Urban       Hall     GA  524.0    False        Low   \n3138              Rural    Buffalo     NE  100.0     True        Low   \n3139  MediumSmall_Urban  Henderson     KY   98.0    False     Medium   \n\n         region  \n0     Northeast  \n1     Northeast  \n2     Northeast  \n3     Northeast  \n4     Northeast  \n...         ...  \n3135      South  \n3136      South  \n3137      South  \n3138    Midwest  \n3139      South  \n\n[3140 rows x 63 columns]\n\n\nFollowing this, we performed an exploratory data analysis to understand the distribution of the data and the relationships between different variables.\n\n\nShow the code\nchd=cdc\n\n#Scatter plot for a variety of variables against variable of interest\n# Create a 3x4 grid of subplots (12 subplots in total)\nnum_rows = 3\nnum_cols = 4\n\n# Adjust the figure size according to your preference\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 8))\n\n# Flatten the axes array for easier iteration\naxes = axes.ravel()\n\ncol_list = ('Stroke', 'Diabetes', 'Smoker', 'PhysInactivity',\n            'HealthIns', 'HospCR', 'Pharmacies', 'AirQuality',\n            'Poverty', 'MedHouseIncome', 'UrbanRural', 'Parks'\n            )\n\n# Plot each independent variable against the dependent variable in each subplot\nfor i, ax in enumerate(axes):\n    ax.scatter(chd[col_list[i]], chd.CHD, s=1, alpha=0.1)\n    ax.set_title(f\"{col_list[i]}\")\n    ax.set_ylabel(\"CHD\")\n\n# Adjust spacing between subplots for better visualization\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\nimport matplotlib.pyplot as plt2\n\n# Create a heatmap with automatically binned continuous variables and average Z value in each bin\ndef create_heatmap(x_data, y_data, z_data, x_label, y_label, z_label):\n    # Combine the data into a DataFrame\n    data = pd.DataFrame({'X': x_data, 'Y': y_data, 'Z': z_data})\n\n    # Determine the number of bins (you can adjust this as needed)\n    num_bins = 20\n\n    # Use pandas cut function to create bins for X and Y variables\n    data['X_bin'] = pd.cut(data['X'], bins=num_bins, labels=False)\n    data['Y_bin'] = pd.cut(data['Y'], bins=num_bins, labels=False)\n\n    # Group the data by the bins and calculate the average Z value in each bin\n    heatmap_data = data.groupby(['X_bin', 'Y_bin'])['Z'].mean().unstack()\n\n    # Create the heatmap using a separate figure\n    plt2.figure(figsize=(8, 6))\n    sns.heatmap(heatmap_data, cmap='viridis', annot=False, fmt=\".2f\", cbar_kws={'label': z_label})\n    plt2.xlabel(x_label)\n    plt2.ylabel(y_label)\n    plt2.title(f'Heatmap of {z_label} by {x_label} and {y_label}')\n    plt2.show()\n\ncreate_heatmap(chd.PhysInactivity, chd.Poverty, chd.CHD, 'PhysInactivity', 'Poverty', 'Avg. CHD')\n\n\n\n\n\n\n\n\nNext, we split the data into a training set and a test set. We trained SVM and Random Forest models using the training data, tuning their hyperparameters with GridSearchCV and RandomizedSearchCV respectively. In addition to these models, we also employed a k-means clustering algorithm to explore potential clusters in the dataset.\nRandom Forest:\n\n\nShow the code\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import randint\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nchd = pd.read_csv(\"data/CDC_python_clean.csv\")\n\n### Drop unneeded columns\ncolumns_to_drop = ['fips', 'CHD', 'county','UrbanRural']\nchd = chd.drop(columns=columns_to_drop)\n\n### Establish X and y\nX = chd.drop(columns=['CHD_Class'])\ny = chd['CHD_Class']\n\n### Additional preprocessing\n# Get the character columns that need to be dummy encoded\nchar_cols = X.select_dtypes(include=['object']).columns\n\n# Create a one-hot encoder and apply it to the character columns\nencoder = OneHotEncoder()\nX = pd.get_dummies(X, columns=char_cols, drop_first=True)\n\n# Scale and Center\nnumeric_columns = X.select_dtypes(include=['float64', 'int64']).columns\nscaler = StandardScaler()\nX[numeric_columns] = scaler.fit_transform(X[numeric_columns])\n\n### Split data\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=42)\n\n### Establish Parameters\nrf_classifier = RandomForestClassifier()\n\nparam_dist = {\n    'n_estimators': randint(1, 1000),\n    'max_depth': randint(1, 50),\n    'min_samples_split': randint(2, 10),\n    'min_samples_leaf': randint(1, 10),\n    'bootstrap': [True, False],\n    'criterion': ['gini', 'entropy']\n}\n\n### Additional Parameters\nrandom_search = RandomizedSearchCV(\n    estimator=rf_classifier,\n    param_distributions=param_dist,\n    n_iter=50,  # Adjust the number of iterations as needed\n    cv=5,        # Number of cross-validation folds\n    n_jobs=-1,   # Use all available CPU cores for parallel processing\n    random_state=42,\n    verbose=True\n)\n\n#Model\nrandom_search.fit(Xtrain, ytrain)\n\n### Store best model\nbest_rf_classifier = random_search.best_estimator_\n\naccuracy = best_rf_classifier.score(Xtest, ytest)\nprint(\"Accuracy of the Random Forest Classifier: {:.2f}%\".format(accuracy * 100))\n\n#Feature Importance\nfeature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': best_rf_classifier.feature_importances_})\n\ntop_20_features = feature_importance_df.nlargest(50, 'Importance')\n\nimport matplotlib.pyplot as plt\n\n# Create a horizontal bar plot with rotated axes\nplt.figure(figsize=(8, 12))\nplt.barh(top_20_features['Feature'], top_20_features['Importance'], color='darkgreen')\nplt.gca().invert_yaxis()  # Invert the y-axis to show features at the top\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.title('Top 50 Variable Importance in Random Forest Classifier')\nplt.tight_layout()\nplt.show()\n\n\nfrom sklearn.metrics import confusion_matrix\n\nypred = best_rf_classifier.predict(Xtest)\n\nconf_matrix = confusion_matrix(ytest, ypred)\n\nclass_names = ['low', 'medium', 'high']\nconf_matrix_df = pd.DataFrame(conf_matrix, index=class_names, columns=class_names)\n\n## Confussion matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Set the labels for the heatmap\nlabels = ['High','Medium','Low']\n\n# Create the heatmap\nplt.figure(figsize=(6, 4))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix Heatmap')\nplt.show()\n\n### Split Urban Rural and Redo Analysis\ndef get_variable_importance(data):\n    # Separate the target variable 'CHD_Class' from the features\n    X = data.drop(columns=['CHD_Class'])\n    y = data['CHD_Class']\n    \n    # Get the character columns that need to be dummy encoded\n    char_cols = X.select_dtypes(include=['object']).columns\n\n    # Create a one-hot encoder and apply it to the character columns\n    encoder = OneHotEncoder()\n    X = pd.get_dummies(X, columns=char_cols, drop_first=True)\n\n    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=42)\n    \n    rf_classifier = RandomForestClassifier()\n\n    param_dist = {\n        'n_estimators': randint(1, 1000),\n        'max_depth': randint(1, 50),\n        'min_samples_split': randint(2, 10),\n        'min_samples_leaf': randint(1, 10),\n        'bootstrap': [True, False],\n        'criterion': ['gini', 'entropy']\n    }\n    \n    \n    random_search = RandomizedSearchCV(\n        estimator=rf_classifier,\n        param_distributions=param_dist,\n        n_iter=50,  # Adjust the number of iterations as needed\n        cv=5,        # Number of cross-validation folds\n        n_jobs=-1,   # Use all available CPU cores for parallel processing\n        random_state=42,\n        verbose=True\n    )\n\n    random_search.fit(Xtrain, ytrain)\n    \n    best_rf_classifier = random_search.best_estimator_\n    \n    accuracy = best_rf_classifier.score(Xtest, ytest)\n    print(\"Accuracy of the Random Forest Classifier: {:.2f}%\".format(accuracy * 100))\n    \n    # Create a DataFrame to store the feature names and their corresponding importance scores\n    feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': best_rf_classifier.feature_importances_})\n\n    # Sort the DataFrame in descending order based on importance\n    feature_importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n\n    return feature_importance_df\n  \n# Create separate datasets for IsRural=True and IsRural=False\ndata_rural = chd[chd['IsRural'] == True]\ndata_non_rural = chd[chd['IsRural'] == False]\n\n# Get variable importance for IsRural=True\nresult_rural = get_variable_importance(data_rural)\n\n# Get variable importance for IsRural=False\nresult_non_rural = get_variable_importance(data_non_rural)\n\n# Add a new column to each DataFrame to indicate the dataset (IsRural=True or IsRural=False)\nresult_rural['Dataset'] = 'IsRural=True'\nresult_non_rural['Dataset'] = 'IsRural=False'\n\n# Concatenate the two DataFrames into a single DataFrame\ncombined_result = pd.concat([result_rural, result_non_rural,], ignore_index=True)\n\ncombined_result = combined_result[combined_result['Importance'] > 0.01]\n\n# Create a bar plot with rotated axes\nplt.figure(figsize=(10, 10))\nsns.barplot(x='Importance', y='Feature', hue='Dataset', data=combined_result)\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.title('Variable Importance Comparison between IsRural=True and IsRural=False')\nplt.legend(title='Dataset', loc='lower right')\nplt.tight_layout()\nplt.show()\n\n\nFitting 5 folds for each of 50 candidates, totalling 250 fits\n\n\nAccuracy of the Random Forest Classifier: 89.43%\n\n\n\n\n\n\n\n\nFitting 5 folds for each of 50 candidates, totalling 250 fits\n\n\nAccuracy of the Random Forest Classifier: 88.66%\nFitting 5 folds for each of 50 candidates, totalling 250 fits\n\n\nAccuracy of the Random Forest Classifier: 88.70%\n\n\n\n\n\nK-Means:\n\n\nShow the code\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\ndf = pd.read_csv('data/CDC_python_clean.csv')\ndf = df.loc[df.CHD > 0]\n\n### Selecting columns\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Select numerical columns\nnumerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n\n# Remove the 'fips' column as it's a unique identifier, not a meaningful numerical feature\nnumerical_cols = numerical_cols.drop('fips')\n\n# Subset the dataframe on these columns\ndf_numerical = df[numerical_cols]\n\n# Normalize the data\nscaler = StandardScaler()\ndf_normalized = pd.DataFrame(scaler.fit_transform(df_numerical), columns=df_numerical.columns)\n\ndf_normalized.head()\n\n### Determining how many clusters to use\n\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Set a range of clusters to try out\nclusters_range = range(2, 15)\n\n# List to hold the inertia for each number of clusters\ninertias = []\n\n# Perform K-means for each number of clusters and store the inertia\nfor num_clusters in clusters_range:\n    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n    kmeans.fit(df_normalized)\n    inertias.append(kmeans.inertia_)\n\n# Plot the elbow plot\nplt.figure(figsize=(8, 6))\nplt.plot(clusters_range, inertias, 'bo-')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Inertia')\nplt.title('Elbow Method For Optimal Number of Clusters')\nplt.grid(True)\nplt.show()\n\n# The Elbow Method plot shows the inertia (sum of squared distances to the nearest cluster center) as a function of the number of clusters. From the plot, it's not entirely clear where the \"elbow\" is, as there isn't a sharp bend. This often happens with real-world data, which may not have well-separated clusters. However, we can see that the inertia starts to decrease at a slower rate from around 4 clusters onwards. Therefore, we'll choose 4 as the number of clusters for our K-means clustering.\n\n# Perform K-means clustering with 4 clusters\nkmeans = KMeans(n_clusters=4, random_state=42)\nkmeans.fit(df_normalized)\n\n# Get the cluster assignments for each data point\ncluster_assignments = kmeans.labels_\n\n# Add the cluster assignments back to the original DataFrame\ndf['Cluster'] = cluster_assignments\n\ndf.head()\n\n# Calculate the mean values of our features within each cluster\ncluster_characteristics = df.groupby('Cluster').mean()\ncluster_characteristics.transpose()\n\nfrom sklearn.metrics import silhouette_score\n\n# Calculate the silhouette score\nsil_score = silhouette_score(df_normalized, cluster_assignments)\n\nsil_score\n\n# The silhouette score for our clustering is approximately 0.167. This score is relatively low, indicating that the clusters are not very clearly separated and that the samples within each cluster are not extremely dense. This is not surprising considering that we have used a high-dimensional dataset with many features, which can make it challenging to form distinct clusters.\n\n\n\n\n\n0.16659025768870228\n\n\nSVC:\n\n\nShow the code\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n# Load data\nchd = pd.read_csv(\"data/CDC_python_clean.csv\")\n\n# Drop unnecessary columns\ncolumns_to_drop = ['fips', 'CHD', 'county', 'UrbanRural']\nchd = chd.drop(columns=columns_to_drop)\n\n# Define features and target\nX = chd.drop(columns=['CHD_Class'])\ny = chd['CHD_Class']\n\n# One-hot encode categorical variables\nchar_cols = X.select_dtypes(include=['object']).columns\nX = pd.get_dummies(X, columns=char_cols, drop_first=True)\n\n# Scale numerical variables\nnumeric_columns = X.select_dtypes(include=['float64', 'int64']).columns\nscaler = StandardScaler()\nX[numeric_columns] = scaler.fit_transform(X[numeric_columns])\n\nX.head(), y.head()\n\n# Splitting the data\n\nfrom sklearn.model_selection import train_test_split\n\n# Split data into train and test sets\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=42)\n\nXtrain.shape, Xtest.shape\n\n#Model Building and Hyperparameter Tuning\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\n# Define the SVC classifier\nsvc = SVC(kernel='linear')\n\n# Define the parameter grid for the GridSearchCV\nparam_grid = {\n    'C': [0.1, 1],\n}\n\n# Define the GridSearchCV for hyperparameter tuning\ngrid_search = GridSearchCV(\n    estimator=svc,\n    param_grid=param_grid,\n    cv=5,        # Number of cross-validation folds\n    n_jobs=-1,   # Use all available CPU cores for parallel processing\n    verbose=True\n)\n\n# Fit the GridSearchCV to the training data\ngrid_search.fit(Xtrain, ytrain)\n\n# Model Evaluation\n\nfrom sklearn.metrics import accuracy_score\n\n# Get the best SVC classifier\nbest_svc = grid_search.best_estimator_\n\n# Predict the test set results\nsvc_pred = best_svc.predict(Xtest)\n\n# Compute the accuracy of the SVC classifier\nsvc_accuracy = accuracy_score(ytest, svc_pred)\n\nsvc_accuracy\n\n# Feature Importance Plot\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Get feature importances (use absolute values of coefficients as proxy)\nimportances = abs(best_svc.coef_[0])\n\n# Get top 10 features\nindices = np.argsort(importances)[-10:]\n\nplt.figure(figsize=(12, 8))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [X.columns[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()\n\n# Confusion Matrix and Plot\n# Import the required libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Use the sklearn function confusion_matrix to compute the confusion matrix\nsvc_cm = confusion_matrix(ytest, svc_pred)\n\n# Using seaborn to create a heatmap. The heatmap will visually represent the confusion matrix\n# Each cell in the heatmap corresponds to a cell in the confusion matrix.\n# The color of the cell is proportional to the number of instances.\nsns.heatmap(svc_cm, annot=True, fmt='d')\n\n# Label the x-axis as 'Predicted'\nplt.xlabel('Predicted')\n\n# Label the y-axis as 'True'\nplt.ylabel('True')\n\n\nFitting 5 folds for each of 2 candidates, totalling 10 fits\n\n\n\n\n\nText(55.333333333333336, 0.5, 'True')\n\n\n\n\n\nThe performance of the SVM and Random Forest models was evaluated based on accuracy on the test data, while the k-means clustering results were assessed visually and qualitatively.\n\n\nShow the code\nsvc_accuracy = accuracy_score(ytest, svc_pred)\nprint(\"Accuracy of the Support Vector Classifier: {:.2f}%\".format(svc_accuracy * 100))\naccuracy = best_rf_classifier.score(Xtest, ytest)\nprint(\"Accuracy of the Random Forest Classifier: {:.2f}%\".format(accuracy * 100))\nsil_score = silhouette_score(df_normalized, cluster_assignments)\nprint(\"Accuracy of the K-Means Cluster: {:.2f}%\".format(sil_score * 100))\n\n\nAccuracy of the Support Vector Classifier: 92.10%\nAccuracy of the Random Forest Classifier: 89.43%\nAccuracy of the K-Means Cluster: 16.66%"
  },
  {
    "objectID": "posts/Python Group Project/CHD Final Report.html#results",
    "href": "posts/Python Group Project/CHD Final Report.html#results",
    "title": "Healthcare Access and its Effects on Coronary Heart Disease Prevalence",
    "section": "Results:",
    "text": "Results:\nThrough our exploratory data analysis, we found interesting patterns and distributions in the data, such as, you can see a positive correlation between coronary heart disease (CHD) and people who smoke, and with people who have had a stroke. Also see that as poverty rates increase there is an association that prevalence of CHD also goes up. When you look at median household income, as income rises prevalence of coronary heart disease goes down.\nOur machine learning models yielded promising results. The SVM model achieved an accuracy of approximately 92.1%, while the Random Forest model achieved an accuracy of approximately 89.4%. Therefore, the SVM model performed slightly better on this dataset. The k-means clustering shows a silhouette score of approximately 0.167. This score is relatively low, indicating that the clusters are not very clearly separated and that the samples within each cluster are not extremely dense. This is not surprising considering that we have used a high-dimensional dataset with many features, which can make it challenging to form distinct clusters.."
  },
  {
    "objectID": "posts/Python Group Project/CHD Final Report.html#discussions-and-implications",
    "href": "posts/Python Group Project/CHD Final Report.html#discussions-and-implications",
    "title": "Healthcare Access and its Effects on Coronary Heart Disease Prevalence",
    "section": "Discussions and Implications:",
    "text": "Discussions and Implications:\nThe difference in performance between the SVM and Random Forest models could be attributed to the specific properties of these algorithms. SVMs tend to perform well on high-dimensional data, while Random Forests are often better suited for datasets with a mix of categorical and numerical features. The k-means clustering results provide additional insights into the structure of the dataset, potentially informing feature engineering and selection processes.\nThese findings highlight the potential of machine learning in aiding the prediction of Coronary Heart Disease, which could have significant implications for early intervention and treatment planning."
  },
  {
    "objectID": "posts/Python Group Project/CHD Final Report.html#conclusion",
    "href": "posts/Python Group Project/CHD Final Report.html#conclusion",
    "title": "Healthcare Access and its Effects on Coronary Heart Disease Prevalence",
    "section": "Conclusion:",
    "text": "Conclusion:\nIn conclusion, our project demonstrates the feasibility and potential of using machine learning for predicting Coronary Heart Disease. The findings suggest that machine learning, and specifically SVM, can provide a valuable tool in the field of health informatics. Future work could explore other algorithms, feature engineering techniques, and larger or more diverse datasets to further improve prediction performance.These findings suggest that machine learning can be a valuable tool in health informatics, providing insights that can aid in early disease prediction and intervention."
  },
  {
    "objectID": "posts/Python Group Project/CHD Final Report.html#references",
    "href": "posts/Python Group Project/CHD Final Report.html#references",
    "title": "Healthcare Access and its Effects on Coronary Heart Disease Prevalence",
    "section": "References:",
    "text": "References:\n\nScikit-learn: Machine Learning in Python (https://scikit-learn.org/stable/index.html)\nCDC Dataset (Link to CDC Dataset)"
  },
  {
    "objectID": "posts/Python Group Project/CHD Final Report.html#appendix-list-of-contributors",
    "href": "posts/Python Group Project/CHD Final Report.html#appendix-list-of-contributors",
    "title": "Healthcare Access and its Effects on Coronary Heart Disease Prevalence",
    "section": "Appendix: List of Contributors",
    "text": "Appendix: List of Contributors\n\nHans LehnDorff - LinkedIn\nIsaac Johnson - LinkedIn\nJesse Debolt - LinkedIn"
  }
]